{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilykruger/Documents/GitHub/CSH-Internship/thesis_venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/emilykruger/Documents/GitHub/CSH-Internship/thesis_venv/lib/python3.9/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "#sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) # for scripts\n",
    "project_root = '/Users/emilykruger/Documents/GitHub/CSH-Internship'\n",
    "functions_dir = os.path.join(project_root, 'src/functions')\n",
    "daegc_dir = os.path.join(project_root, 'src/DAEGC')\n",
    "\n",
    "sys.path.append(project_root) #for local notebook\n",
    "sys.path.append(functions_dir) #for local notebook\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "import regex as re\n",
    "from src.functions.linguistic_features import remove_emojis, remove_tags, count_emojis, preprocess_text, count_pos_tags\n",
    "from textstat import flesch_reading_ease\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "import multiprocessing as mp\n",
    "\n",
    "# to get cluster labels\n",
    "import torch\n",
    "from src.DAEGC.DAEGC import DAEGC\n",
    "from functions.daegc_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Code for Linguistic Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, code will be written to extract linguistic features from the dataset. It will be done on a small subsample. Afterwards code will be transferred to a script to run on the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = pd.read_csv('../data/selected_groups_with_transcriptions.csv.gzip', compression='gzip')\n",
    "channels = pd.read_csv('../data/channel_subsample.csv.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = groups.drop(columns=['Unnamed: 0'], axis=1)\n",
    "groups['group_or_channel'] = 'group'\n",
    "groups.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = channels.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'], axis=1)\n",
    "channels['group_or_channel'] = 'channel'\n",
    "channels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take random sample of 100k rows of both df where either message or fwd_message contains data and combine\n",
    "sample_groups = groups[groups['message'].notnull() | groups['fwd_message'].notnull()].sample(n=1000, random_state=42)\n",
    "sample_channels = channels = channels[channels['message'].notnull() | channels['fwd_message'].notnull()].sample(n=1000, random_state=42)\n",
    "combined = pd.concat([sample_groups, sample_channels], ignore_index=True, axis=0)\n",
    "combined.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only UID and message\n",
    "messages = combined[['UID_key', 'message', 'fwd_message', 'group_or_channel']]\n",
    "\n",
    "#remove emojis\n",
    "cleaned_messages = []\n",
    "for message in messages['message'].astype(str):\n",
    "    cleaned_messages.append(remove_emojis(message))\n",
    "\n",
    "cleaned_fwd_messages = []\n",
    "for message in messages['fwd_message'].astype(str):\n",
    "    cleaned_fwd_messages.append(remove_emojis(message))\n",
    "\n",
    "messages['message_string'] = cleaned_messages\n",
    "messages['fwd_message_string'] = cleaned_fwd_messages\n",
    "messages['message_string'] = messages['message_string'].astype(str)\n",
    "messages['fwd_message_string'] = messages['fwd_message_string'].astype(str)\n",
    "\n",
    "#if message, take message else take fwd_message\n",
    "messages['final_message'] = messages['message'].where(messages['message'].notnull(), messages['fwd_message'])\n",
    "messages['final_message_string'] = messages['message_string'].where(messages['message_string'] != 'nan', messages['fwd_message_string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['preprocessed_message'] = messages['final_message_string'].apply(preprocess_text)\n",
    "\n",
    "#delete uneccessary columns\n",
    "messages = messages.drop(columns=['message', 'fwd_message', 'message_string', 'fwd_message_string'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.to_csv('../data/messages_sample.csv.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Re-Running Below Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for re-running\n",
    "messages = pd.read_csv('../data/samples/messages_sample_2000.csv.gzip', compression='gzip').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count-Based Features & POS-Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num sentences\n",
    "messages['sent_count'] = messages['final_message_string'].apply(lambda x: len(re.split(r'[.!?]+', x)) if x else 0)\n",
    "#num words\n",
    "messages['word_count'] = messages['final_message_string'].apply(lambda x: len(re.findall(r'\\w+', x)) if x else 0)\n",
    "#avg sentence length (words per sentence)\n",
    "messages['avg_sent_length'] = messages.apply(lambda row: row['word_count'] / row['sent_count'] if row['sent_count'] > 0 else 0, axis=1)\n",
    "#avg word length (characters per word)\n",
    "messages['avg_word_length'] = messages.apply(lambda row: len(row['final_message_string'].replace(' ', '')) / row['word_count'] if row['word_count'] > 0 else 0, axis=1)\n",
    "#num exclamations (multiple ! coutn as one exclamation)\n",
    "messages['exclamation_count'] = messages['final_message_string'].apply(lambda x: len(re.findall(r'!+', x)) if x else 0)\n",
    "#num questions (multiple ? count as one question)\n",
    "messages['question_count'] = messages['final_message_string'].apply(lambda x: len(re.findall(r'\\?+', x)) if x else 0)\n",
    "#num emojis \n",
    "messages['emoji_count'] = messages['final_message'].apply(lambda x: count_emojis(x) if x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use count_pos_tags func to count nouns, verbs and adj\n",
    "messages['noun_count'] = messages['final_message_string'].apply(lambda x: count_pos_tags(x)[0])\n",
    "messages['verb_count'] = messages['final_message_string'].apply(lambda x: count_pos_tags(x)[1])\n",
    "messages['adj_count'] = messages['final_message_string'].apply(lambda x: count_pos_tags(x)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flesch Reading Ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use TextStat to compute Flesch Reading Ease score on final_message_string\n",
    "messages['flesch_reading_ease'] = messages['final_message_string'].apply(flesch_reading_ease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.head(5)\n",
    "messages.to_csv('../data/messages_with_features.csv.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Complexity Classifier Model Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, pipeline, DistilBertForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('MiriUll/distilbert-german-text-complexity')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('MiriUll/distilbert-german-text-complexity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Mit solchen Drohungen kommt sie nie mehr zurück \", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "predicted_class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-classification\", model=\"MiriUll/distilbert-german-text-complexity\")\n",
    "print(pipe('Das ist ein einfacher Satz.'))\n",
    "print(pipe('Obwohl der junge Wissenschaftler sich intensiv auf seine Forschungsarbeit konzentrierte, war er oft von den unvorhersehbaren und lauten Bauarbeiten im Nachbargebäude abgelenkt, die seine produktivsten Stunden regelmäßig störten.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Emoji Sentiment Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = pd.read_csv('../data/archive/Emoji_Sentiment_Data_v1.0.csv')\n",
    "#emoji sentiment column based on max value of positive neutral or negative\n",
    "emojis['sentiment'] = emojis[['Positive', 'Neutral', 'Negative']].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-formatting Liwc Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/LIWC2007_German.dic'\n",
    "skiprows = 70  # Specify the number of rows to skip\n",
    "\n",
    "data = []\n",
    "\n",
    "with open(file_path, 'r', encoding='latin1') as file:\n",
    "    # Step 1: Skip the specified number of rows\n",
    "    for _ in range(skiprows):\n",
    "        next(file)\n",
    "    \n",
    "    # Read the file line-by-line\n",
    "    for line in file:\n",
    "        split_line = line.strip().split('\\t')\n",
    "        word = split_line[0]\n",
    "        categories = split_line[1:]\n",
    "        data.append([word, categories])\n",
    "\n",
    "# Step 2: Create DataFrame with flexible columns\n",
    "# Define headers\n",
    "headers = ['word', 'categories']\n",
    "\n",
    "# Step 3: Create DataFrame\n",
    "df = pd.DataFrame(data, columns=headers)\n",
    "\n",
    "# Print the DataFrame to check the result\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.explode('categories')\n",
    "df['categories'] = df['categories'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_categories = {\n",
    "    1: 'Pronoun',\n",
    "    2: 'I',\n",
    "    3: 'We',\n",
    "    4: 'Self',\n",
    "    5: 'You',\n",
    "    6: 'Other',\n",
    "    7: 'Negate',\n",
    "    8: 'Assent',\n",
    "    9: 'Article',\n",
    "    10: 'Preps',\n",
    "    11: 'Number',\n",
    "    12: 'Affect',\n",
    "    13: 'Posemo',\n",
    "    14: 'Posfeel',\n",
    "    15: 'Optim',\n",
    "    16: 'Negemo',\n",
    "    17: 'Anx',\n",
    "    18: 'Anger',\n",
    "    19: 'Sad',\n",
    "    20: 'Cogmech',\n",
    "    21: 'Cause',\n",
    "    22: 'Insight',\n",
    "    23: 'Discrep',\n",
    "    24: 'Inhib',\n",
    "    25: 'Tentat',\n",
    "    26: 'Certain',\n",
    "    27: 'Senses',\n",
    "    28: 'See',\n",
    "    29: 'Hear',\n",
    "    30: 'Feel',\n",
    "    31: 'Social',\n",
    "    32: 'Comm',\n",
    "    33: 'Othref',\n",
    "    34: 'Friends',\n",
    "    35: 'Family',\n",
    "    36: 'Humans',\n",
    "    37: 'Time',\n",
    "    38: 'Past',\n",
    "    39: 'Present',\n",
    "    40: 'Future',\n",
    "    41: 'Space',\n",
    "    42: 'Up',\n",
    "    43: 'Down',\n",
    "    44: 'Incl',\n",
    "    45: 'Excl',\n",
    "    46: 'Motion',\n",
    "    47: 'Occup',\n",
    "    48: 'School',\n",
    "    49: 'Job',\n",
    "    50: 'Achieve',\n",
    "    51: 'Leisure',\n",
    "    52: 'Home',\n",
    "    53: 'Sports',\n",
    "    54: 'TV',\n",
    "    55: 'Music',\n",
    "    56: 'Money',\n",
    "    57: 'Metaph',\n",
    "    58: 'Relig',\n",
    "    59: 'Death',\n",
    "    60: 'Physcal',\n",
    "    61: 'Body',\n",
    "    62: 'Sexual',\n",
    "    63: 'Eating',\n",
    "    64: 'Sleep',\n",
    "    65: 'Groom',\n",
    "    66: 'Swear',\n",
    "    67: 'Nonfl',\n",
    "    68: 'Fillers'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cat_name'] = df['categories'].map(liwc_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the order of the columns so that its word, cat_name, categories\n",
    "df = df[['word', 'cat_name', 'categories']]\n",
    "\n",
    "#write df to txt file but omit index and column header\n",
    "df.to_csv('../data/liwc_german_2007.txt', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making txt file for GAWK script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "filename = 'messages_sample_10'\n",
    "sample = pd.read_csv(f'../data/samples/{filename}.csv.gzip', compression='gzip').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keep UID_key and final_message_string and save as txt without \"\" around messages\n",
    "\n",
    "sample = sample[['UID_key', 'final_message_string']]\n",
    "sample.to_csv(f'../data/samples/{filename}.txt', sep='\\t', index=False, header=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing sampling strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10 #how big of a sample to take from each dataset\n",
    "random_state = 42\n",
    "\n",
    "########## LOAD AND PREPARE DATASET ##########\n",
    "\n",
    "#load two datasets, drop unnecessary columns and add column to indicate group or channel\n",
    "groups = pd.read_csv('../data/selected_groups_with_transcriptions.csv.gzip', compression='gzip').drop(columns=['Unnamed: 0'], axis=1)\n",
    "channels = pd.read_csv('../data/channel_subsample.csv.gzip', compression='gzip').drop(columns=['Unnamed: 0', 'Unnamed: 0.1'], axis=1)\n",
    "\n",
    "\n",
    "groups['group_or_channel'] = 'group'\n",
    "channels['group_or_channel'] = 'channel'\n",
    "\n",
    "\n",
    "#take random sample of both df where either message or fwd_message (or transcribedmessage if group) contains data and combine\n",
    "sample_groups = groups[groups['message'].notnull() | groups['fwd_message'].notnull() | groups['transcribed_message'].notnull()].sample(n=sample_size, random_state=random_state)\n",
    "sample_channels = channels = channels[channels['message'].notnull() | channels['fwd_message'].notnull()].sample(n=sample_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([sample_groups, sample_channels], ignore_index=True, axis=0)\n",
    "\n",
    "#keep only necessary columns\n",
    "messages = combined[['UID_key', 'message', 'fwd_message', 'transcribed_message', 'group_or_channel']]\n",
    "\n",
    "#remove emojis and links\n",
    "cleaned_messages = []\n",
    "for message in messages['message'].astype(str):\n",
    "    message = remove_tags(message)\n",
    "    cleaned_messages.append(remove_emojis(message))\n",
    "\n",
    "cleaned_fwd_messages = []\n",
    "for message in messages['fwd_message'].astype(str):\n",
    "    message = remove_tags(message)\n",
    "    cleaned_fwd_messages.append(remove_emojis(message))\n",
    "\n",
    "messages['message_string'] = cleaned_messages\n",
    "messages['fwd_message_string'] = cleaned_fwd_messages\n",
    "messages['message_string'] = messages['message_string'].astype(str)\n",
    "messages['fwd_message_string'] = messages['fwd_message_string'].astype(str)\n",
    "\n",
    "#if message, take message else take fwd_message else take transcribed message\n",
    "messages['final_message'] = np.where(messages['message'].notnull(), messages['message'],\n",
    "                                    np.where(messages['fwd_message'].notnull(), messages['fwd_message'],\n",
    "                                             messages['transcribed_message'])).astype(str)\n",
    "messages['final_message_string'] = np.where(messages['message_string'] != 'nan', messages['message_string'],\n",
    "                                    np.where(messages['fwd_message_string'] != 'nan', messages['fwd_message_string'],\n",
    "                                             messages['transcribed_message'])).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['final_message_string'] = messages['final_message_string'].apply(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gawk -f ../src/analysis/liwc_category_ratios.awk ../data/liwc_german_2007.txt ../data/samples/messages_sample_200.txt > ../results/liwc_ratios.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the output file but remove last column\n",
    "liwc_ratios = pd.read_csv('../results/liwc_ratios.csv', sep=',')\n",
    "liwc_ratios = liwc_ratios.iloc[:, :-1]\n",
    "liwc_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ling_features = pd.read_csv('../results/messages_with_features_200.csv.gzip', compression='gzip').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ling_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat liwc_ratios and ling_features based on UID_key\n",
    "merged = pd.merge(ling_features, liwc_ratios, on='UID_key', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "sentiment_model = pipeline(model=\"aari1995/German_Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"Ich liebe die Bahn. Pünktlich wie immer ... -.-\",\"Krasser Service\"]\n",
    "result = sentiment_model(sentence)\n",
    "print(result)\n",
    "#Output:\n",
    "#[{'label': 'negative', 'score': 0.4935680031776428},{'label': 'positive', 'score': 0.5790663957595825}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Assuming 'sentiment_model' is already loaded\n",
    "# Load the tokenizer corresponding to your sentiment model\n",
    "tokenizer = AutoTokenizer.from_pretrained('aari1995/German_Sentiment')  # Replace 'model_name' with the actual model name\n",
    "\n",
    "sentiment_aari = []\n",
    "\n",
    "for message in messages['final_message_string']:\n",
    "    # Encode the message, truncate to max length of the model, and only keep the input_ids\n",
    "    inputs = tokenizer.encode(message, return_tensors='pt', max_length=512, truncation=True)\n",
    "    # Decode back to text string, to feed into the sentiment model as expected\n",
    "    truncated_message = tokenizer.decode(inputs[0], skip_special_tokens=True)\n",
    "    result = sentiment_model(truncated_message)\n",
    "    sentiment_aari.append(result[0]['label'])\n",
    "\n",
    "messages['sentiment_aari'] = sentiment_aari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sent = []\n",
    "neg_sent = []\n",
    "neutral_sent = []\n",
    "\n",
    "for message in tqdm(messages['final_message_string'], desc = 'Extracting Sentiment'):\n",
    "    # if message is empty, don't calculate sentiment\n",
    "    if message == '' or message == 'nan':\n",
    "        pos_sent.append(np.nan)\n",
    "        neg_sent.append(np.nan)\n",
    "        neutral_sent.append(np.nan)\n",
    "    else:\n",
    "        # encode & decode message and truncate to max length that model can handle\n",
    "        result = sentiment_model(message[:512])\n",
    "        sent = (result[0]['label'])\n",
    "        if sent == 'positive':\n",
    "            pos_sent.append(1)\n",
    "            neg_sent.append(0)\n",
    "            neutral_sent.append(0)\n",
    "        elif sent == 'negative':\n",
    "            pos_sent.append(0)\n",
    "            neg_sent.append(1)\n",
    "            neutral_sent.append(0)\n",
    "        elif sent == 'neutral':\n",
    "            pos_sent.append(0)\n",
    "            neg_sent.append(0)\n",
    "            neutral_sent.append(1)\n",
    "        else:\n",
    "            pos_sent.append(np.nan)\n",
    "            neg_sent.append(np.nan)\n",
    "            neutral_sent.append(np.nan)\n",
    "\n",
    "messages['positive_sentiment'] = pos_sent\n",
    "messages['negative_sentiment'] = neg_sent\n",
    "messages['neutral_sentiment'] = neutral_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Initialize sentiment lists\n",
    "pos_sent = [np.nan] * len(messages['final_message_string'])\n",
    "neg_sent = [np.nan] * len(messages['final_message_string'])\n",
    "neutral_sent = [np.nan] * len(messages['final_message_string'])\n",
    "\n",
    "# Map sentiment labels to list indices\n",
    "sentiment_map = {\n",
    "    'positive': (1, 0, 0),\n",
    "    'negative': (0, 1, 0),\n",
    "    'neutral': (0, 0, 1)\n",
    "}\n",
    "\n",
    "# Process messages\n",
    "for idx, message in tqdm(enumerate(messages['final_message_string']), desc='Extracting Sentiment', total=len(messages['final_message_string'])):\n",
    "    # Skip empty messages\n",
    "    if message in ('', 'nan'):\n",
    "        continue\n",
    "\n",
    "    # Run sentiment analysis\n",
    "    result = sentiment_model(message[:512])  # Use the pipeline directly with the message text\n",
    "    sent = result[0]['label']\n",
    "\n",
    "    # Update sentiment lists\n",
    "    if sent in sentiment_map:\n",
    "        pos_sent[idx], neg_sent[idx], neutral_sent[idx] = sentiment_map[sent]\n",
    "\n",
    "# Assign results back to DataFrame\n",
    "messages['positive_sentiment'] = pos_sent\n",
    "messages['negative_sentiment'] = neg_sent\n",
    "messages['neutral_sentiment'] = neutral_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in tqdm(messages['final_message_string']):\n",
    "    sentiment = sentiment_model(message[:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "text = \"Erneuter Streik in der S-Bahn\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained('ssary/XLM-RoBERTa-German-sentiment')\n",
    "tokenizer = AutoTokenizer.from_pretrained('ssary/XLM-RoBERTa-German-sentiment')\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "sentiment_classes = ['negative', 'neutral', 'positive']\n",
    "print(sentiment_classes[predictions.argmax()]) # for the class with highest probability\n",
    "print(predictions) # for each class probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = pd.read_csv('../data/samples/messages_sample_200.csv.gzip', compression='gzip').drop('Unnamed: 0', axis=1)\n",
    "messages['final_message_string'] = messages['final_message_string'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict sentiment on all messages\n",
    "sentiment = []\n",
    "neg_prob = []\n",
    "neu_prob = []\n",
    "pos_prob = []\n",
    "\n",
    "for message in messages['final_message_string']:\n",
    "    inputs = tokenizer(message, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    sentiment_classes = ['negative', 'neutral', 'positive']\n",
    "    sentiment.append(sentiment_classes[predictions.argmax()])\n",
    "    neg_prob.append(predictions[0][0].item())\n",
    "    neu_prob.append(predictions[0][1].item())\n",
    "    pos_prob.append(predictions[0][2].item())\n",
    "\n",
    "messages['sentiment'] = sentiment\n",
    "messages['neg_prob'] = neg_prob\n",
    "messages['neu_prob'] = neu_prob\n",
    "messages['pos_prob'] = pos_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print all messages with their sentiment\n",
    "for i, row in messages.iterrows():\n",
    "    print(f'{row[\"final_message_string\"]} - {row[\"sentiment\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages[['final_message_string', 'sentiment', 'sentiment_aari']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print all messages with their sentiment\n",
    "for i, row in messages.iterrows():\n",
    "    print(f'{row[\"final_message_string\"]}\\nRoberta: {row[\"sentiment\"]}\\nAari: {row[\"sentiment_aari\"]}\\n', '-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/samples/messages_sample_200.csv.gzip', compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = pd.read_csv('../data/channel_subsample.csv.gzip', compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Perspective API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from googleapiclient import discovery\n",
    "import json\n",
    "from config import API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = discovery.build(\n",
    "\"commentanalyzer\",\n",
    "\"v1alpha1\",\n",
    "developerKey=API_KEY,\n",
    "discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "static_discovery=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../results/post-aggregation/author_200.csv.gzip', compression = 'gzip')\n",
    "df['final_message_string'] = df['final_message_string'].astype(str)\n",
    "df['toxicity'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toxicity_detection(sentences, client):\n",
    "    toxic = []\n",
    "    for sent in sentences:\n",
    "        analyze_request = {\n",
    "            'comment': { 'text': f\"{sent}\" },\n",
    "            'languages' : [\"de\"],\n",
    "            'requestedAttributes': {'TOXICITY': {}},\n",
    "        }\n",
    "\n",
    "        response = client.comments().analyze(body=analyze_request).execute()\n",
    "        j = json.dumps(response, indent=2)\n",
    "        #print(json.loads(j)['attributeScores']['TOXICITY']['summaryScore']['value'])\n",
    "        toxic.append(json.loads(j)['attributeScores']['TOXICITY']['summaryScore']['value'])\n",
    "    avg = sum(toxic)/len(toxic)\n",
    "    print(avg)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def toxicity_detection(sentences):\n",
    "#     toxic = []\n",
    "#     for sent in sentences:\n",
    "#         analyze_request = {\n",
    "#             'comment': { 'text': f\"{sent}\" },\n",
    "#             'languages' : [\"de\"],\n",
    "#             'requestedAttributes': {'TOXICITY': {}},\n",
    "#         }\n",
    "\n",
    "#         response = client.comments().analyze(body=analyze_request).execute()\n",
    "#         j = json.dumps(response, indent=2)\n",
    "#         #print(json.loads(j)['attributeScores']['TOXICITY']['summaryScore']['value'])\n",
    "#         toxic.append(json.loads(j)['attributeScores']['TOXICITY']['summaryScore']['value'])\n",
    "#     return sum(toxic)/len(toxic)\n",
    "\n",
    "\n",
    "# # n= 10000\n",
    "# # list_df = [sample[i:i+n] for i in range(0,len(sample),n)]\n",
    "\n",
    "\n",
    "# #final_toxic_list = []\n",
    "# # for df in list_df:\n",
    "# for i in tqdm(range(len(sample_df))):\n",
    "#     row = sample_df.iloc[i]\n",
    "#     #toxic = []\n",
    "#     if row['toxicity'] == 0: \n",
    "\n",
    "#         tmp = [sent.strip() for sent in re.split(r'[.!?]', row.final_message_string) if len(sent.split()) > 5]\n",
    "\n",
    "#         if (len(tmp) > 100):\n",
    "#             tmp = random.sample(tmp, 100)\n",
    "#         if (len(tmp) > 1):\n",
    "#             row['toxicity'] = toxicity_detection(tmp)\n",
    "\n",
    "#     sample_df.at[i, 'toxicity'] = row['toxicity']\n",
    "\n",
    "#     #df.at[i, 'toxicity'] = toxic\n",
    "#     #final_toxic_list.append(df)\n",
    "\n",
    "# # con = pd.concat(final_toxic_list)\n",
    "# # con.to_csv('fa_toxic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split df into chunks\n",
    "n= 20\n",
    "list_df = [df[i:i+n] for i in range(0,len(df),n)]\n",
    "\n",
    "#iterate over chunks and rows to extract toxicity score\n",
    "final_toxic_list = []\n",
    "for df in list_df:\n",
    "    for i in tqdm(range(len(df))):\n",
    "        row = df.iloc[i]\n",
    "        if row['toxicity'] == 0: \n",
    "            #split message into list of sentences to pass to toxicity detection function\n",
    "            tmp = [sent.strip() for sent in re.split(r'[.!?]', row['final_message_string']) if len(sent.split()) > 5]\n",
    "\n",
    "            if (len(tmp) > 100):\n",
    "                tmp = random.sample(tmp, 100)\n",
    "            #print(tmp)\n",
    "            if (len(tmp) > 1):\n",
    "                row['toxicity'] = toxicity_detection(tmp, client)\n",
    "            else:\n",
    "                print('no sentence')\n",
    "        df.at[df.index[i], 'toxicity'] = row['toxicity']\n",
    "        print('df.at...', df.at[df.index[i], 'toxicity'])\n",
    "    final_toxic_list.append(df)\n",
    "\n",
    "#concat chunks\n",
    "df_after = pd.concat(final_toxic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['final_message_string'] == 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_toxic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_after[df_after['toxicity'] != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('../results/post-aggregation/author_200.csv.gzip', compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['toxicity'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forwarded Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df[sample_df['forwarded_message'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample = pd.read_csv('../data/samples/messages_sample_200.csv.gzip', compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample[new_sample['forwarded_message'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample['final_message_string'] = new_sample['final_message_string'].astype(str)\n",
    "new_sample['final_message'] = new_sample['final_message'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample['sent_count'] = new_sample['final_message_string'].apply(lambda x: len(re.split(r'[.!?]+', x)) if x != '' and x != 'nan' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample['question_count'] = new_sample['final_message_string'].apply(lambda x: len(re.findall(r'\\?+', x)) if x else 0)\n",
    "#num emojis \n",
    "new_sample['emoji_count'] = new_sample['final_message'].apply(lambda x: count_emojis(x) if x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = []\n",
    "verbs = []\n",
    "adjectives = []\n",
    "\n",
    "\n",
    "for message in tqdm(new_sample['final_message_string'], desc = 'Extracting POS Tag counts'):\n",
    "        noun, verb, adj = count_pos_tags(message)\n",
    "        nouns.append(noun)\n",
    "        verbs.append(verb)\n",
    "        adjectives.append(adj)\n",
    "                        \n",
    "new_sample['noun_count'] = nouns\n",
    "new_sample['verb_count'] = verbs\n",
    "new_sample['adj_count'] = adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 200\n",
    "pre_agg = pd.read_csv(f'../results/pre-aggregation/liwcANDfeatures_results_{sample_size}.csv.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_agg = pd.get_dummies(pre_agg, columns=['group_or_channel', 'flesch_reading_ease_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_agg.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation dictionary\n",
    "agg_dict = {\n",
    "    # COUNT\n",
    "    'UID_key': 'count',\n",
    "\n",
    "    # SUM\n",
    "    'own_message': 'sum',\n",
    "    'forwarded_message': 'sum',\n",
    "    'noun_count': 'sum',\n",
    "    'verb_count': 'sum',\n",
    "    'adj_count': 'sum',\n",
    "    'positive_sentiment': 'sum',\n",
    "    'negative_sentiment': 'sum',\n",
    "    'neutral_sentiment': 'sum',\n",
    "    'group_or_channel_channel': 'sum',\n",
    "    'group_or_channel_group': 'sum',\n",
    "    'flesch_reading_ease_class_difficult': 'sum',\n",
    "    'flesch_reading_ease_class_easy': 'sum',\n",
    "    'flesch_reading_ease_class_fairly difficult': 'sum',\n",
    "    'flesch_reading_ease_class_fairly easy': 'sum',\n",
    "    'flesch_reading_ease_class_standard': 'sum',\n",
    "    'flesch_reading_ease_class_unclassified': 'sum',\n",
    "    'flesch_reading_ease_class_very confusing': 'sum',\n",
    "    'flesch_reading_ease_class_very easy': 'sum',\n",
    "\n",
    "    # AVG\n",
    "    'sent_count': 'mean',\n",
    "    'word_count': 'mean',\n",
    "    'avg_sent_length': 'mean',\n",
    "    'avg_word_length': 'mean',\n",
    "    'exclamation_count': 'mean',\n",
    "    'question_count': 'mean',\n",
    "    'emoji_count': 'mean',\n",
    "    'flesch_reading_ease': 'mean',\n",
    "    'liwc_I': 'mean',\n",
    "    'liwc_We': 'mean',\n",
    "    'liwc_You': 'mean',\n",
    "    'liwc_Other': 'mean',\n",
    "    'liwc_Affect': 'mean',\n",
    "    \n",
    "    # ' '.JOIN\n",
    "    'fwd_message': lambda x: ' '.join(x.dropna().astype(str)),\n",
    "    'fwd_message_string': lambda x: ' '.join(x.dropna().astype(str)),\n",
    "    'final_message': lambda x: ' '.join(x.dropna().astype(str)),\n",
    "    'final_message_string': lambda x: ' '.join(x.dropna().astype(str)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict for aggregatopn\n",
    "agg = pre_agg.groupby(['author', 'date']).agg(agg_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(df):    \n",
    "    df['sent_count'] = df['final_message_string'].apply(lambda x: len(re.split(r'[.!?]+', x)) if x != '' and x != 'nan' else 0)\n",
    "    #num words\n",
    "    df['word_count'] = df['final_message_string'].apply(lambda x: len(re.findall(r'\\w+', x)) if x != '' and x != 'nan' else 0)\n",
    "    #avg sentence length (words per sentence)\n",
    "    df['avg_sent_length'] = df.apply(lambda row: row['word_count'] / row['sent_count'] if row['sent_count'] > 0 else 0, axis=1)\n",
    "    #avg word length (characters per word)\n",
    "    df['avg_word_length'] = df.apply(lambda row: len(row['final_message_string'].replace(' ', '')) / row['word_count'] if row['word_count'] > 0 else 0, axis=1)\n",
    "    #num exclamations (multiple ! coutn as one exclamation)\n",
    "    df['exclamation_count'] = df['final_message_string'].apply(lambda x: len(re.findall(r'!+', x)) if x else 0)\n",
    "    #num questions (multiple ? count as one question)\n",
    "    df['question_count'] = df['final_message_string'].apply(lambda x: len(re.findall(r'\\?+', x)) if x else 0)\n",
    "    #num emojis \n",
    "    df['emoji_count'] = df['final_message'].apply(lambda x: count_emojis(x) if x else 0)\n",
    "\n",
    "    print('Simple count based features extracted.')\n",
    "\n",
    "    ########## COUNT OF SELECTED POS TAGS ##########\n",
    "\n",
    "    #count nouns, verbs and adj\n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    adjectives = []\n",
    "\n",
    "    for message in tqdm(df['final_message_string'], desc = 'Extracting POS Tag counts'):\n",
    "        noun, verb, adj = count_pos_tags(message)\n",
    "        nouns.append(noun)\n",
    "        verbs.append(verb)\n",
    "        adjectives.append(adj)\n",
    "                        \n",
    "    df['noun_count'] = nouns\n",
    "    df['verb_count'] = verbs\n",
    "    df['adj_count'] = adjectives\n",
    "\n",
    "    ########## FLESCH READING EASE SCORE ##########\n",
    "\n",
    "    textstat.set_lang('de')\n",
    "    #compute Flesch Reading Ease score on non-empty df\n",
    "    df['flesch_reading_ease'] = df['final_message_string'].apply(lambda x: textstat.flesch_reading_ease(x) if x.strip() != '' and x != 'nan' else np.nan)\n",
    "\n",
    "    #classify scores based on: https://pypi.org/project/textstat/\n",
    "    flesch_classes = []\n",
    "    for score in df['flesch_reading_ease']:\n",
    "        if score >= 0 and score < 30:\n",
    "            flesch_classes.append('very confusing')\n",
    "        elif score >= 30 and score < 50:\n",
    "            flesch_classes.append('difficult')\n",
    "        elif score >= 50 and score < 60:\n",
    "            flesch_classes.append('fairly difficult')\n",
    "        elif score >=60 and score < 70:\n",
    "            flesch_classes.append('standard')\n",
    "        elif score >=70 and score < 80:\n",
    "            flesch_classes.append('fairly easy')\n",
    "        elif score >=80 and score < 90:\n",
    "            flesch_classes.append('easy')\n",
    "        elif score >=90 and score < 101:\n",
    "            flesch_classes.append('very easy')\n",
    "        else:\n",
    "            flesch_classes.append('unclassified')\n",
    "        \n",
    "    df['flesch_reading_ease_class'] = flesch_classes\n",
    "\n",
    "    print('Flesch Reading Ease score extracted.')\n",
    "\n",
    "    ########## SENTIMENT ANALYSIS ##########\n",
    "\n",
    "    #load tokenizer and sentiment model\n",
    "    print('Loading sentiment model...')\n",
    "    sentiment_model = pipeline(model='aari1995/German_Sentiment')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('aari1995/German_Sentiment')  \n",
    "\n",
    "    pos_sent = []\n",
    "    neg_sent = []\n",
    "    neutral_sent = []\n",
    "\n",
    "    for message in tqdm(df['final_message_string'], desc = 'Extracting Sentiment'):\n",
    "        #if message is empty, don't calculate sentiment\n",
    "        if message == '' or message == 'nan':\n",
    "            pos_sent.append(np.nan)\n",
    "            neg_sent.append(np.nan)\n",
    "            neutral_sent.append(np.nan)\n",
    "        else:\n",
    "            #truncate message to max length model can handle\n",
    "            result = sentiment_model(message[:512])\n",
    "            sent = (result[0]['label'])\n",
    "            if sent == 'positive':\n",
    "                pos_sent.append(1)\n",
    "                neg_sent.append(0)\n",
    "                neutral_sent.append(0)\n",
    "            elif sent == 'negative':\n",
    "                pos_sent.append(0)\n",
    "                neg_sent.append(1)\n",
    "                neutral_sent.append(0)\n",
    "            elif sent == 'neutral':\n",
    "                pos_sent.append(0)\n",
    "                neg_sent.append(0)\n",
    "                neutral_sent.append(1)\n",
    "            else:\n",
    "                pos_sent.append(np.nan)\n",
    "                neg_sent.append(np.nan)\n",
    "                neutral_sent.append(np.nan)\n",
    "\n",
    "    df['positive_sentiment'] = pos_sent\n",
    "    df['negative_sentiment'] = neg_sent\n",
    "    df['neutral_sentiment'] = neutral_sent\n",
    "    print('Sentiment extracted.')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_cluster_metrics(n_cores, network_dict_list):\n",
    "    rep_list = []\n",
    "\n",
    "    pool = Pool(n_cores)\n",
    "\n",
    "    for result in tqdm(\n",
    "        pool.imap_unordered(func=calculate_cluster_results, iterable=network_dict_list),\n",
    "        total=len(network_dict_list)\n",
    "        ):\n",
    "            rep_list.append(result)\n",
    "\n",
    "    pool.close()\n",
    "    return rep_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataframe into n_cores parts and return list of dicts\n",
    "def split_df(n_cores, df):\n",
    "    df_list = np.array_split(df, n_cores)\n",
    "    return [df_part.to_dict('records') for df_part in df_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cores=4\n",
    "df_list = split_df(4, new_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pool_cluster_metrics(n_cores, df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Aggregation Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'../data/aggregated/author_date_{sample_size}.csv.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df[df['message_count'] > 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of count columns to convert to percentages\n",
    "count_columns = [\n",
    "    'own_message',\n",
    "    'forwarded_message',\n",
    "    'positive_sentiment',\n",
    "    'negative_sentiment',\n",
    "    'neutral_sentiment',\n",
    "    'group_or_channel_channel',\n",
    "    'group_or_channel_group',\n",
    "    'flesch_reading_ease_class_difficult',\n",
    "    'flesch_reading_ease_class_easy',\n",
    "    'flesch_reading_ease_class_fairly difficult',\n",
    "    'flesch_reading_ease_class_fairly easy',\n",
    "    'flesch_reading_ease_class_standard',\n",
    "    'flesch_reading_ease_class_unclassified',\n",
    "    'flesch_reading_ease_class_very confusing',\n",
    "    'flesch_reading_ease_class_very easy'\n",
    "]\n",
    "\n",
    "# Convert counts to percentages row by row\n",
    "for index, row in df.iterrows():\n",
    "    for col in count_columns:\n",
    "        df.at[index, col] = row[col] / row['message_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../results/post-aggregation/author_date_200.csv.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test['message_count'] > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelization - Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/samples/messages_sample_200.csv.gzip', compression='gzip')\n",
    "df_non = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(df):    \n",
    "    ########## FEATURE EXTRACTION ##########\n",
    "\n",
    "    #num sentences\n",
    "    df['sent_count'] = df['final_message_string'].apply(lambda x: len(re.split(r'[.!?]+', x)) if x != '' and x != 'nan' else 0)\n",
    "    #num words\n",
    "    df['word_count'] = df['final_message_string'].apply(lambda x: len(re.findall(r'\\w+', x)) if x != '' and x != 'nan' else 0)\n",
    "    #avg sentence length (words per sentence)\n",
    "    df['avg_sent_length'] = df.apply(lambda row: row['word_count'] / row['sent_count'] if row['sent_count'] > 0 else 0, axis=1)\n",
    "    #avg word length (characters per word)\n",
    "    df['avg_word_length'] = df.apply(lambda row: len(row['final_message_string'].replace(' ', '')) / row['word_count'] if row['word_count'] > 0 else 0, axis=1)\n",
    "    #num exclamations (multiple ! coutn as one exclamation)\n",
    "    df['exclamation_count'] = df['final_message_string'].apply(lambda x: len(re.findall(r'!+', x)) if x else 0)\n",
    "    #num questions (multiple ? count as one question)\n",
    "    df['question_count'] = df['final_message_string'].apply(lambda x: len(re.findall(r'\\?+', x)) if x else 0)\n",
    "    #num emojis \n",
    "    df['emoji_count'] = df['final_message'].apply(lambda x: count_emojis(x) if x else 0)\n",
    "\n",
    "    print('Simple count based features extracted.')\n",
    "\n",
    "    ########## COUNT OF SELECTED POS TAGS ##########\n",
    "\n",
    "    #count nouns, verbs and adj\n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    adjectives = []\n",
    "\n",
    "    for message in tqdm(df['final_message_string'], desc = 'Extracting POS Tag counts'):\n",
    "        noun, verb, adj = count_pos_tags(message)\n",
    "        nouns.append(noun)\n",
    "        verbs.append(verb)\n",
    "        adjectives.append(adj)\n",
    "                        \n",
    "    df['noun_count'] = nouns\n",
    "    df['verb_count'] = verbs\n",
    "    df['adj_count'] = adjectives\n",
    "\n",
    "    ########## FLESCH READING EASE SCORE ##########\n",
    "\n",
    "    textstat.set_lang('de')\n",
    "    #compute Flesch Reading Ease score on non-empty df\n",
    "    df['flesch_reading_ease'] = df['final_message_string'].apply(lambda x: textstat.flesch_reading_ease(x) if x.strip() != '' and x != 'nan' else np.nan)\n",
    "\n",
    "    #classify scores based on: https://pypi.org/project/textstat/\n",
    "    flesch_classes = []\n",
    "    for score in df['flesch_reading_ease']:\n",
    "        if score >= 0 and score < 30:\n",
    "            flesch_classes.append('very confusing')\n",
    "        elif score >= 30 and score < 50:\n",
    "            flesch_classes.append('difficult')\n",
    "        elif score >= 50 and score < 60:\n",
    "            flesch_classes.append('fairly difficult')\n",
    "        elif score >=60 and score < 70:\n",
    "            flesch_classes.append('standard')\n",
    "        elif score >=70 and score < 80:\n",
    "            flesch_classes.append('fairly easy')\n",
    "        elif score >=80 and score < 90:\n",
    "            flesch_classes.append('easy')\n",
    "        elif score >=90 and score < 101:\n",
    "            flesch_classes.append('very easy')\n",
    "        else:\n",
    "            flesch_classes.append('unclassified')\n",
    "        \n",
    "    df['flesch_reading_ease_class'] = flesch_classes\n",
    "\n",
    "    print('Flesch Reading Ease score extracted.')\n",
    "\n",
    "    ########## SENTIMENT ANALYSIS ##########\n",
    "\n",
    "    #load tokenizer and sentiment model\n",
    "    print('Loading sentiment model...')\n",
    "    sentiment_model = pipeline(model='aari1995/German_Sentiment')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('aari1995/German_Sentiment')  \n",
    "\n",
    "    pos_sent = []\n",
    "    neg_sent = []\n",
    "    neutral_sent = []\n",
    "\n",
    "    for message in tqdm(df['final_message_string'], desc = 'Extracting Sentiment'):\n",
    "        #if message is empty, don't calculate sentiment\n",
    "        if message == '' or message == 'nan':\n",
    "            pos_sent.append(np.nan)\n",
    "            neg_sent.append(np.nan)\n",
    "            neutral_sent.append(np.nan)\n",
    "        else:\n",
    "            #truncate message to max length model can handle\n",
    "            result = sentiment_model(message[:512])\n",
    "            sent = (result[0]['label'])\n",
    "            if sent == 'positive':\n",
    "                pos_sent.append(1)\n",
    "                neg_sent.append(0)\n",
    "                neutral_sent.append(0)\n",
    "            elif sent == 'negative':\n",
    "                pos_sent.append(0)\n",
    "                neg_sent.append(1)\n",
    "                neutral_sent.append(0)\n",
    "            elif sent == 'neutral':\n",
    "                pos_sent.append(0)\n",
    "                neg_sent.append(0)\n",
    "                neutral_sent.append(1)\n",
    "            else:\n",
    "                pos_sent.append(np.nan)\n",
    "                neg_sent.append(np.nan)\n",
    "                neutral_sent.append(np.nan)\n",
    "\n",
    "    df['positive_sentiment'] = pos_sent\n",
    "    df['negative_sentiment'] = neg_sent\n",
    "    df['neutral_sentiment'] = neutral_sent\n",
    "    print('Sentiment extracted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df, func, num_partitions):\n",
    "    # Split the dataframe into smaller chunks\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    # Create a pool of workers\n",
    "    with mp.Pool(num_partitions) as pool:\n",
    "        # Apply the function to each chunk\n",
    "        for df in pool.map(func, df_split):\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split = np.array_split(df, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start_non_parallel = time.time()\n",
    "########## FEATURE EXTRACTION ##########\n",
    "\n",
    "#num sentences\n",
    "df_non['sent_count'] = df_non['final_message_string'].apply(lambda x: len(re.split(r'[.!?]+', x)) if x != '' and x != 'nan' else 0)\n",
    "#num words\n",
    "df_non['word_count'] = df_non['final_message_string'].apply(lambda x: len(re.findall(r'\\w+', x)) if x != '' and x != 'nan' else 0)\n",
    "#avg sentence length (words per sentence)\n",
    "df_non['avg_sent_length'] = df_non.apply(lambda row: row['word_count'] / row['sent_count'] if row['sent_count'] > 0 else 0, axis=1)\n",
    "#avg word length (characters per word)\n",
    "df_non['avg_word_length'] = df_non.apply(lambda row: len(row['final_message_string'].replace(' ', '')) / row['word_count'] if row['word_count'] > 0 else 0, axis=1)\n",
    "#num exclamations (multiple ! coutn as one exclamation)\n",
    "df_non['exclamation_count'] = df_non['final_message_string'].apply(lambda x: len(re.findall(r'!+', x)) if x else 0)\n",
    "#num questions (multiple ? count as one question)\n",
    "df_non['question_count'] = df_non['final_message_string'].apply(lambda x: len(re.findall(r'\\?+', x)) if x else 0)\n",
    "#num emojis \n",
    "df_non['emoji_count'] = df_non['final_message'].apply(lambda x: count_emojis(x) if x else 0)\n",
    "\n",
    "print('Simple count based features extracted.')\n",
    "\n",
    "########## COUNT OF SELECTED POS TAGS ##########\n",
    "\n",
    "#count nouns, verbs and adj\n",
    "nouns = []\n",
    "verbs = []\n",
    "adjectives = []\n",
    "\n",
    "for message in tqdm(df_non['final_message_string'], desc = 'Extracting POS Tag counts'):\n",
    "    noun, verb, adj = count_pos_tags(message)\n",
    "    nouns.append(noun)\n",
    "    verbs.append(verb)\n",
    "    adjectives.append(adj)\n",
    "                    \n",
    "df_non['noun_count'] = nouns\n",
    "df_non['verb_count'] = verbs\n",
    "df_non['adj_count'] = adjectives\n",
    "\n",
    "########## FLESCH READING EASE SCORE ##########\n",
    "\n",
    "textstat.set_lang('de')\n",
    "#compute Flesch Reading Ease score on non-empty df_non\n",
    "df_non['flesch_reading_ease'] = df_non['final_message_string'].apply(lambda x: textstat.flesch_reading_ease(x) if x.strip() != '' and x != 'nan' else np.nan)\n",
    "\n",
    "#classify scores based on: https://pypi.org/project/textstat/\n",
    "flesch_classes = []\n",
    "for score in df_non['flesch_reading_ease']:\n",
    "    if score >= 0 and score < 30:\n",
    "        flesch_classes.append('very confusing')\n",
    "    elif score >= 30 and score < 50:\n",
    "        flesch_classes.append('difficult')\n",
    "    elif score >= 50 and score < 60:\n",
    "        flesch_classes.append('fairly difficult')\n",
    "    elif score >=60 and score < 70:\n",
    "        flesch_classes.append('standard')\n",
    "    elif score >=70 and score < 80:\n",
    "        flesch_classes.append('fairly easy')\n",
    "    elif score >=80 and score < 90:\n",
    "        flesch_classes.append('easy')\n",
    "    elif score >=90 and score < 101:\n",
    "        flesch_classes.append('very easy')\n",
    "    else:\n",
    "        flesch_classes.append('unclassified')\n",
    "    \n",
    "df_non['flesch_reading_ease_class'] = flesch_classes\n",
    "\n",
    "print('Flesch Reading Ease score extracted.')\n",
    "\n",
    "########## SENTIMENT ANALYSIS ##########\n",
    "\n",
    "#load tokenizer and sentiment model\n",
    "print('Loading sentiment model...')\n",
    "sentiment_model = pipeline(model='aari1995/German_Sentiment')\n",
    "tokenizer = AutoTokenizer.from_pretrained('aari1995/German_Sentiment')  \n",
    "\n",
    "pos_sent = []\n",
    "neg_sent = []\n",
    "neutral_sent = []\n",
    "\n",
    "for message in tqdm(df_non['final_message_string'], desc = 'Extracting Sentiment'):\n",
    "    #if message is empty, don't calculate sentiment\n",
    "    if message == '' or message == 'nan':\n",
    "        pos_sent.append(np.nan)\n",
    "        neg_sent.append(np.nan)\n",
    "        neutral_sent.append(np.nan)\n",
    "    else:\n",
    "        #truncate message to max length model can handle\n",
    "        result = sentiment_model(message[:512])\n",
    "        sent = (result[0]['label'])\n",
    "        if sent == 'positive':\n",
    "            pos_sent.append(1)\n",
    "            neg_sent.append(0)\n",
    "            neutral_sent.append(0)\n",
    "        elif sent == 'negative':\n",
    "            pos_sent.append(0)\n",
    "            neg_sent.append(1)\n",
    "            neutral_sent.append(0)\n",
    "        elif sent == 'neutral':\n",
    "            pos_sent.append(0)\n",
    "            neg_sent.append(0)\n",
    "            neutral_sent.append(1)\n",
    "        else:\n",
    "            pos_sent.append(np.nan)\n",
    "            neg_sent.append(np.nan)\n",
    "            neutral_sent.append(np.nan)\n",
    "\n",
    "df_non['positive_sentiment'] = pos_sent\n",
    "df_non['negative_sentiment'] = neg_sent\n",
    "df_non['neutral_sentiment'] = neutral_sent\n",
    "print('Sentiment extracted.')\n",
    "\n",
    "time_end_non_parallel = time.time()\n",
    "print(f'Non-parallel execution time: {time_end_non_parallel - time_start_non_parallel} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = pd.read_csv(f'../data/samples/messages_sample_200.csv.gzip', compression='gzip').drop(columns=['Unnamed: 0'], axis=1)\n",
    "messages['final_message_string'] = messages['final_message_string'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and sentiment model\n",
    "print('Loading sentiment model...')\n",
    "sentiment_model = pipeline(model='aari1995/German_Sentiment')\n",
    "tokenizer = AutoTokenizer.from_pretrained('aari1995/German_Sentiment')\n",
    "\n",
    "# Define a function to process a single message\n",
    "def analyze_sentiment(message):\n",
    "    if message == '' or message == 'nan':\n",
    "        return np.nan, np.nan, np.nan\n",
    "    else:\n",
    "        # Truncate message to max length model can handle\n",
    "        result = sentiment_model(message[:512])\n",
    "        sent = result[0]['label']\n",
    "        if sent == 'positive':\n",
    "            return 1, 0, 0\n",
    "        elif sent == 'negative':\n",
    "            return 0, 1, 0\n",
    "        elif sent == 'neutral':\n",
    "            return 0, 0, 1\n",
    "        else:\n",
    "            return np.nan, np.nan, np.nan\n",
    "\n",
    "# Initialize lists to store sentiment results\n",
    "pos_sent = []\n",
    "neg_sent = []\n",
    "neutral_sent = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ThreadPoolExecutor to parallelize sentiment analysis\n",
    "print('Starting sentiment extraction...')\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    # Submit tasks and store futures\n",
    "    futures = [executor.submit(analyze_sentiment, msg) for msg in messages['final_message_string']]\n",
    "    # Process results as they become available\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc='Extracting Sentiment'):\n",
    "        pos, neg, neutral = future.result()\n",
    "        pos_sent.append(pos)\n",
    "        neg_sent.append(neg)\n",
    "        neutral_sent.append(neutral)\n",
    "\n",
    "# Add sentiment results to the DataFrame\n",
    "messages['positive_sentiment'] = pos_sent\n",
    "messages['negative_sentiment'] = neg_sent\n",
    "messages['neutral_sentiment'] = neutral_sent\n",
    "print('Sentiment extraction done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# was_forwarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 200\n",
    "pre_agg = pd.read_csv(f'../results/pre-aggregation/liwcANDfeatures_results_{sample_size}.csv.gzip', compression='gzip')\n",
    "print('Dataset loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_agg = pd.get_dummies(pre_agg, columns=['group_or_channel'])\n",
    "print('Dummies for categorial variables created.')\n",
    "messages = pre_agg[['author', 'own_message', 'forwarded_message', 'fwd_author', 'UID_key', 'group_name', 'date']]\n",
    "pre_agg = pre_agg[pre_agg['own_message'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dict = {\n",
    "    # SUM\n",
    "    'noun_count': 'sum',\n",
    "    'verb_count': 'sum',\n",
    "    'adj_count': 'sum',\n",
    "    'positive_sentiment': 'sum',\n",
    "    'negative_sentiment': 'sum',\n",
    "    'neutral_sentiment': 'sum',\n",
    "    'group_or_channel_channel': 'sum',\n",
    "    'group_or_channel_group': 'sum',\n",
    "\n",
    "    # AVG\n",
    "    'sent_count': 'mean',\n",
    "    'word_count': 'mean',\n",
    "    'avg_sent_length': 'mean',\n",
    "    'avg_word_length': 'mean',\n",
    "    'exclamation_count': 'mean',\n",
    "    'question_count': 'mean',\n",
    "    'emoji_count': 'mean',\n",
    "    'flesch_reading_ease': 'mean',\n",
    "    'liwc_I': 'mean',\n",
    "    'liwc_We': 'mean',\n",
    "    'liwc_You': 'mean',\n",
    "    'liwc_Other': 'mean',\n",
    "    'liwc_Affect': 'mean',\n",
    "    \n",
    "    # ' '.JOIN\n",
    "    'final_message': lambda x: ' '.join(x.dropna().astype(str)),\n",
    "    'final_message_string': lambda x: ' '.join(x.dropna().astype(str)),\n",
    "}\n",
    "\n",
    "# Aggregation dictionary for message ratios\n",
    "agg_dict_messages = {\n",
    "    'own_message': 'sum',\n",
    "    'forwarded_message': 'sum',\n",
    "    'UID_key': 'count'\n",
    "}\n",
    "\n",
    "########## RENAMING COLUMNS ##########\n",
    "\n",
    "rename_dict = {'group_or_channel_channel': 'channel_messages', 'group_or_channel_group': 'group_messages', 'UID_key': 'total_message_count'}\n",
    "\n",
    "\n",
    "print('Aggregating per author and group...')\n",
    "#aggregate linguistic features\n",
    "agg_author_group = pre_agg.groupby(['author', 'group_name']).agg(agg_dict)\n",
    "agg_author_group = agg_author_group.rename(columns=rename_dict)\n",
    "#aggregate message ratios\n",
    "agg_author_group_messages = messages.groupby(['author', 'group_name']).agg(agg_dict_messages)\n",
    "agg_author_group_messages = agg_author_group_messages.rename(columns=rename_dict)\n",
    "#concat based on author and group columns\n",
    "agg_author_group = pd.merge(\n",
    "    left = agg_author_group,\n",
    "    right = agg_author_group_messages,\n",
    "    how = 'outer',\n",
    "    left_on = ['author', 'group_name'],\n",
    "    right_on = ['author', 'group_name']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_author_group.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_author_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "sample_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'../data/samples/messages_sample_{sample_size}.csv.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of authors in each group\n",
    "grouped_authors = df.groupby('group_name')['author'].apply(list)\n",
    "\n",
    "# get combinations of two authors in each group\n",
    "edges = []\n",
    "for authors in grouped_authors:\n",
    "    if len(authors) > 1:\n",
    "        edges += combinations(sorted(set(authors)), 2)\n",
    "\n",
    "# count occurences of combo to determine edge weight\n",
    "edge_weights = Counter(edges)\n",
    "\n",
    "# save as df\n",
    "edgelist = pd.DataFrame(edge_weights.items(), columns=['edge', 'weight'])\n",
    "edgelist[['author_1', 'author_2']] = pd.DataFrame(edgelist['edge'].tolist(), index=edgelist.index)\n",
    "edgelist = edgelist.drop(columns='edge')\n",
    "edgelist.to_csv(f'../data/edgelists/author_{sample_size}_edgelist.csv', index=False)\n",
    "\n",
    "# Final weighted edgelist with columns 'author_1', 'author_2', and 'weight'\n",
    "print(edgelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINNALY FIXED Toxicity Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toxicity_detection(message, client):\n",
    "    analyze_request = {\n",
    "        'comment': { 'text': f\"{message}\" },\n",
    "        'languages' : [\"de\"],\n",
    "        'requestedAttributes': {'TOXICITY': {}},\n",
    "    }\n",
    "    response = client.comments().analyze(body=analyze_request).execute()\n",
    "    toxic =response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "    return toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#initialize column\n",
    "toxicity = []\n",
    "\n",
    "for i in tqdm(range(len(results))):\n",
    "    row = results.iloc[i]\n",
    "    message = row['final_message_string']\n",
    "    if row['own_message'] == 1:\n",
    "        tox = toxicity_detection(message, client)\n",
    "        toxicity.append(tox)\n",
    "    else:\n",
    "        toxicity.append(np.nan)\n",
    "\n",
    "results['toxicity'] = toxicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Cluster Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fd/qf26p3js3x32_9_qwsb841y80000gn/T/ipykernel_22926/603577995.py:1: DtypeWarning: Columns (22,23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  features = pd.read_csv('../results/post-aggregation/author_full.csv.gzip', compression='gzip')\n"
     ]
    }
   ],
   "source": [
    "features = pd.read_csv('../results/post-aggregation/author_full.csv.gzip', compression='gzip')\n",
    "# load \"raw\" dataset to analyse adjacency matrix\n",
    "data = pd.read_csv('../data/samples/messages_sample_full.csv.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = DAEGC(30, 128, 9, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = DAEGC(30, 128, 9, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5 = DAEGC(30, 128, 9, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DAEGC(\n",
       "  (gat): GAT(\n",
       "    (conv1): GATLayer (30 -> 128)\n",
       "    (conv2): GATLayer (128 -> 9)\n",
       "  )\n",
       "  (gat_layer1): GATLayer (9 -> 9)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.load_state_dict(torch.load('../model/DAEGC_3Clusters.pkl'))\n",
    "model_3.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DAEGC(\n",
       "  (gat): GAT(\n",
       "    (conv1): GATLayer (30 -> 128)\n",
       "    (conv2): GATLayer (128 -> 9)\n",
       "  )\n",
       "  (gat_layer1): GATLayer (9 -> 9)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4.load_state_dict(torch.load('../model/DAEGC_4Clusters.pkl'))\n",
    "model_4.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DAEGC(\n",
       "  (gat): GAT(\n",
       "    (conv1): GATLayer (30 -> 128)\n",
       "    (conv2): GATLayer (128 -> 9)\n",
       "  )\n",
       "  (gat_layer1): GATLayer (9 -> 9)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model weights\n",
    "model_5.load_state_dict(torch.load('../model/DAEGC_5Clusters.pkl'))\n",
    "model_5.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors: 16885\n",
      "Adjacency tensor shape: torch.Size([16885, 16885])\n",
      "Normalized adjacency tensor shape: torch.Size([16885, 16885])\n"
     ]
    }
   ],
   "source": [
    "adj, adj_norm = create_adj_matrix(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix created.\n",
      "Feature tensor shape: torch.Size([16885, 30])\n"
     ]
    }
   ],
   "source": [
    "x = create_feature_matrix(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix shape: (16885, 16885)\n"
     ]
    }
   ],
   "source": [
    "M = get_M(adj_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get soft embeddings & cluster assignments\n",
    "with torch.no_grad():\n",
    "    _, z_3, q_3 = model_3(x, adj_norm, M)\n",
    "    _, z_4, q_4 = model_4(x, adj_norm, M)\n",
    "    _, z_5, q_5 = model_5(x, adj_norm, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Assignment for 3 Clusters: Counter({3: 11542, 0: 3307, 5: 2036})\n"
     ]
    }
   ],
   "source": [
    "# get cluster assignments\n",
    "q3_labels = torch.argmax(q_3, dim=1)\n",
    "# count number of nodes in each cluster\n",
    "q3_cluster_count = Counter(q3_labels.numpy())\n",
    "print('Cluster Assignment for 3 Clusters:', q3_cluster_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Assignment for 4 Clusters: Counter({3: 11542, 0: 3284, 5: 2034, 1: 25})\n"
     ]
    }
   ],
   "source": [
    "# get cluster assignments\n",
    "q4_labels = torch.argmax(q_4, dim=1)\n",
    "# count number of nodes in each cluster\n",
    "q4_cluster_count = Counter(q4_labels.numpy())\n",
    "print('Cluster Assignment for 4 Clusters:', q4_cluster_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Assignment for 5 Clusters: Counter({3: 11264, 4: 3620, 5: 976, 1: 961, 0: 64})\n"
     ]
    }
   ],
   "source": [
    "# get cluster assignments\n",
    "q5_labels = torch.argmax(q_5, dim=1)\n",
    "# count number of nodes in each cluster\n",
    "q5_cluster_count = Counter(q5_labels.numpy())\n",
    "print('Cluster Assignment for 5 Clusters:', q5_cluster_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add cluster assignments to features\n",
    "features['cluster_3'] = q3_labels.numpy()\n",
    "features['cluster_4'] = q4_labels.numpy()\n",
    "features['cluster_5'] = q5_labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>positive_sentiment</th>\n",
       "      <th>negative_sentiment</th>\n",
       "      <th>neutral_sentiment</th>\n",
       "      <th>channel_messages</th>\n",
       "      <th>group_messages</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_sent_length</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>exclamation_count</th>\n",
       "      <th>question_count</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>liwc_I</th>\n",
       "      <th>liwc_We</th>\n",
       "      <th>liwc_You</th>\n",
       "      <th>liwc_Other</th>\n",
       "      <th>liwc_Affect</th>\n",
       "      <th>final_message</th>\n",
       "      <th>final_message_string</th>\n",
       "      <th>own_message</th>\n",
       "      <th>forwarded_message</th>\n",
       "      <th>total_message_count</th>\n",
       "      <th>was_forwarded</th>\n",
       "      <th>own_message_count</th>\n",
       "      <th>forwarded_message_count</th>\n",
       "      <th>action_quotient</th>\n",
       "      <th>sentiment_quotient</th>\n",
       "      <th>avg_flesch_reading_ease_class</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>cluster_3</th>\n",
       "      <th>cluster_4</th>\n",
       "      <th>cluster_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!!pv--roland--vp!!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>&lt;a href=\"https://youtu.be/i8SOiIPx-KQ\"&gt;https:/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unclassified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!bex</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ANTIFA MOFU</td>\n",
       "      <td>ANTIFA MOFU</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>difficult</td>\n",
       "      <td>0.249241</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>!pv---l. k. ---vp!</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Wohin bist Du denn geflüchtet,  ist doch fast ...</td>\n",
       "      <td>Wohin bist Du denn geflüchtet, ist doch fast ü...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>easy</td>\n",
       "      <td>0.426917</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>!pv---lotti scarlotta ---vp!</td>\n",
       "      <td>26.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>76.500000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>6.595588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>47.725000</td>\n",
       "      <td>0.044972</td>\n",
       "      <td>0.007463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011194</td>\n",
       "      <td>0.056167</td>\n",
       "      <td>Ich wünsche gute \"Verdaulichkeit\". 😉 Blaues Ge...</td>\n",
       "      <td>Ich wünsche gute \"Verdaulichkeit\". Blaues Gemü...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>difficult</td>\n",
       "      <td>0.105663</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>!pv--tom--pv!</td>\n",
       "      <td>95.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.027778</td>\n",
       "      <td>16.222222</td>\n",
       "      <td>4.616204</td>\n",
       "      <td>5.016480</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>82.231143</td>\n",
       "      <td>0.001736</td>\n",
       "      <td>0.004934</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.032971</td>\n",
       "      <td>0.072214</td>\n",
       "      <td>Na ja, andererseits sagte Trump und sein Gefol...</td>\n",
       "      <td>Na ja, andererseits sagte Trump und sein Gefol...</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>3.052632</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>easy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16880</th>\n",
       "      <td>😎arki 😎</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unclassified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16881</th>\n",
       "      <td>🙏🏻❤️tinu❤️🙏🏻</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unclassified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16882</th>\n",
       "      <td>🚨vwievendetta &amp; 🐕</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unclassified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16883</th>\n",
       "      <td>🤓 müller</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unclassified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16884</th>\n",
       "      <td>🥰😝😜🤪</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unclassified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16885 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             author  noun_count  verb_count  adj_count  \\\n",
       "0                !!pv--roland--vp!!         0.0         0.0        0.0   \n",
       "1                              !bex         1.0         0.0        0.0   \n",
       "2                !pv---l. k. ---vp!         1.0         2.0        1.0   \n",
       "3      !pv---lotti scarlotta ---vp!        26.0        12.0       10.0   \n",
       "4                     !pv--tom--pv!        95.0        58.0       19.0   \n",
       "...                             ...         ...         ...        ...   \n",
       "16880                       😎arki 😎         NaN         NaN        NaN   \n",
       "16881                  🙏🏻❤️tinu❤️🙏🏻         NaN         NaN        NaN   \n",
       "16882             🚨vwievendetta & 🐕         NaN         NaN        NaN   \n",
       "16883                      🤓 müller         NaN         NaN        NaN   \n",
       "16884                          🥰😝😜🤪         NaN         NaN        NaN   \n",
       "\n",
       "       positive_sentiment  negative_sentiment  neutral_sentiment  \\\n",
       "0                0.000000            0.000000           0.000000   \n",
       "1                0.000000            1.000000           0.000000   \n",
       "2                0.000000            1.000000           0.000000   \n",
       "3                0.500000            0.000000           0.500000   \n",
       "4                0.222222            0.361111           0.388889   \n",
       "...                   ...                 ...                ...   \n",
       "16880                 NaN                 NaN                NaN   \n",
       "16881                 NaN                 NaN                NaN   \n",
       "16882                 NaN                 NaN                NaN   \n",
       "16883                 NaN                 NaN                NaN   \n",
       "16884                 NaN                 NaN                NaN   \n",
       "\n",
       "       channel_messages  group_messages  sent_count  word_count  \\\n",
       "0                   0.0             1.0    0.000000    0.000000   \n",
       "1                   0.0             1.0    1.000000    2.000000   \n",
       "2                   0.0             1.0    2.000000   14.000000   \n",
       "3                   0.0             1.0    7.500000   76.500000   \n",
       "4                   0.0             1.0    3.027778   16.222222   \n",
       "...                 ...             ...         ...         ...   \n",
       "16880               NaN             NaN         NaN         NaN   \n",
       "16881               NaN             NaN         NaN         NaN   \n",
       "16882               NaN             NaN         NaN         NaN   \n",
       "16883               NaN             NaN         NaN         NaN   \n",
       "16884               NaN             NaN         NaN         NaN   \n",
       "\n",
       "       avg_sent_length  avg_word_length  exclamation_count  question_count  \\\n",
       "0             0.000000         0.000000           0.000000        0.000000   \n",
       "1             2.000000         5.000000           0.000000        0.000000   \n",
       "2             7.000000         4.857143           0.000000        0.000000   \n",
       "3             8.500000         6.595588           0.000000        0.500000   \n",
       "4             4.616204         5.016480           0.555556        0.138889   \n",
       "...                ...              ...                ...             ...   \n",
       "16880              NaN              NaN                NaN             NaN   \n",
       "16881              NaN              NaN                NaN             NaN   \n",
       "16882              NaN              NaN                NaN             NaN   \n",
       "16883              NaN              NaN                NaN             NaN   \n",
       "16884              NaN              NaN                NaN             NaN   \n",
       "\n",
       "       emoji_count  flesch_reading_ease    liwc_I   liwc_We  liwc_You  \\\n",
       "0         0.000000                  NaN  0.000000  0.000000  0.000000   \n",
       "1         0.000000            31.750000  0.000000  0.000000  0.000000   \n",
       "2         0.000000            84.100000  0.000000  0.000000  0.071429   \n",
       "3         2.500000            47.725000  0.044972  0.007463  0.000000   \n",
       "4         0.055556            82.231143  0.001736  0.004934  0.003968   \n",
       "...            ...                  ...       ...       ...       ...   \n",
       "16880          NaN                  NaN       NaN       NaN       NaN   \n",
       "16881          NaN                  NaN       NaN       NaN       NaN   \n",
       "16882          NaN                  NaN       NaN       NaN       NaN   \n",
       "16883          NaN                  NaN       NaN       NaN       NaN   \n",
       "16884          NaN                  NaN       NaN       NaN       NaN   \n",
       "\n",
       "       liwc_Other  liwc_Affect  \\\n",
       "0        0.000000     0.000000   \n",
       "1        0.000000     0.000000   \n",
       "2        0.000000     0.071429   \n",
       "3        0.011194     0.056167   \n",
       "4        0.032971     0.072214   \n",
       "...           ...          ...   \n",
       "16880         NaN          NaN   \n",
       "16881         NaN          NaN   \n",
       "16882         NaN          NaN   \n",
       "16883         NaN          NaN   \n",
       "16884         NaN          NaN   \n",
       "\n",
       "                                           final_message  \\\n",
       "0      <a href=\"https://youtu.be/i8SOiIPx-KQ\">https:/...   \n",
       "1                                     ANTIFA MOFU          \n",
       "2      Wohin bist Du denn geflüchtet,  ist doch fast ...   \n",
       "3      Ich wünsche gute \"Verdaulichkeit\". 😉 Blaues Ge...   \n",
       "4      Na ja, andererseits sagte Trump und sein Gefol...   \n",
       "...                                                  ...   \n",
       "16880                                                NaN   \n",
       "16881                                                NaN   \n",
       "16882                                                NaN   \n",
       "16883                                                NaN   \n",
       "16884                                                NaN   \n",
       "\n",
       "                                    final_message_string  own_message  \\\n",
       "0                                                    NaN     0.571429   \n",
       "1                                           ANTIFA MOFU      1.000000   \n",
       "2      Wohin bist Du denn geflüchtet, ist doch fast ü...     1.000000   \n",
       "3      Ich wünsche gute \"Verdaulichkeit\". Blaues Gemü...     1.000000   \n",
       "4      Na ja, andererseits sagte Trump und sein Gefol...     0.947368   \n",
       "...                                                  ...          ...   \n",
       "16880                                                NaN     0.000000   \n",
       "16881                                                NaN     0.000000   \n",
       "16882                                                NaN     0.000000   \n",
       "16883                                                NaN     0.000000   \n",
       "16884                                                NaN     0.000000   \n",
       "\n",
       "       forwarded_message  total_message_count  was_forwarded  \\\n",
       "0                    0.0                    7              0   \n",
       "1                    0.0                    1              0   \n",
       "2                    0.0                    1              0   \n",
       "3                    0.0                    2              0   \n",
       "4                    0.0                   38              0   \n",
       "...                  ...                  ...            ...   \n",
       "16880                1.0                    1              1   \n",
       "16881                1.0                    1              0   \n",
       "16882                1.0                    2              0   \n",
       "16883                1.0                    1              0   \n",
       "16884                0.0                    6              0   \n",
       "\n",
       "       own_message_count  forwarded_message_count  action_quotient  \\\n",
       "0                      4                        0              NaN   \n",
       "1                      1                        0              NaN   \n",
       "2                      1                        0         2.000000   \n",
       "3                      2                        0         1.200000   \n",
       "4                     36                        0         3.052632   \n",
       "...                  ...                      ...              ...   \n",
       "16880                  0                        1              NaN   \n",
       "16881                  0                        1              NaN   \n",
       "16882                  0                        2              NaN   \n",
       "16883                  0                        1              NaN   \n",
       "16884                  0                        0              NaN   \n",
       "\n",
       "       sentiment_quotient avg_flesch_reading_ease_class  toxicity  cluster_3  \\\n",
       "0                     NaN                  unclassified       NaN          3   \n",
       "1                0.000000                     difficult  0.249241          3   \n",
       "2                0.000000                          easy  0.426917          3   \n",
       "3                     NaN                     difficult  0.105663          0   \n",
       "4                0.615385                          easy       NaN          0   \n",
       "...                   ...                           ...       ...        ...   \n",
       "16880                 NaN                  unclassified       NaN          3   \n",
       "16881                 NaN                  unclassified       NaN          3   \n",
       "16882                 NaN                  unclassified       NaN          3   \n",
       "16883                 NaN                  unclassified       NaN          3   \n",
       "16884                 NaN                  unclassified       NaN          3   \n",
       "\n",
       "       cluster_4  cluster_5  \n",
       "0              3          3  \n",
       "1              3          3  \n",
       "2              3          3  \n",
       "3              0          4  \n",
       "4              0          4  \n",
       "...          ...        ...  \n",
       "16880          3          3  \n",
       "16881          3          3  \n",
       "16882          3          3  \n",
       "16883          3          3  \n",
       "16884          3          3  \n",
       "\n",
       "[16885 rows x 37 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.to_csv('../results/author_full_clusters.csv.gzip', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save node embeddings\n",
    "np.save('../results/author_full_z_3.npy', z_3.numpy())\n",
    "np.save('../results/author_full_z_4.npy', z_4.numpy())\n",
    "np.save('../results/author_full_z_5.npy', z_5.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc3 in position 88888: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../results/author_full_features_and_clusters.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/CSH-Internship/thesis_venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/CSH-Internship/thesis_venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Documents/GitHub/CSH-Internship/thesis_venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/CSH-Internship/thesis_venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/GitHub/CSH-Internship/thesis_venv/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:579\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:668\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2050\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc3 in position 88888: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('../results/author_full_features_and_clusters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fd/qf26p3js3x32_9_qwsb841y80000gn/T/ipykernel_22926/583186708.py:1: DtypeWarning: Columns (22,23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test = pd.read_csv('../results/post-aggregation/author_full.csv.gzip', compression='gzip')\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('../results/post-aggregation/author_full.csv.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author                            object\n",
       "noun_count                       float64\n",
       "verb_count                       float64\n",
       "adj_count                        float64\n",
       "positive_sentiment               float64\n",
       "negative_sentiment               float64\n",
       "neutral_sentiment                float64\n",
       "channel_messages                 float64\n",
       "group_messages                   float64\n",
       "sent_count                       float64\n",
       "word_count                       float64\n",
       "avg_sent_length                  float64\n",
       "avg_word_length                  float64\n",
       "exclamation_count                float64\n",
       "question_count                   float64\n",
       "emoji_count                      float64\n",
       "flesch_reading_ease              float64\n",
       "liwc_I                           float64\n",
       "liwc_We                          float64\n",
       "liwc_You                         float64\n",
       "liwc_Other                       float64\n",
       "liwc_Affect                      float64\n",
       "final_message                     object\n",
       "final_message_string              object\n",
       "own_message                      float64\n",
       "forwarded_message                float64\n",
       "total_message_count                int64\n",
       "was_forwarded                      int64\n",
       "own_message_count                  int64\n",
       "forwarded_message_count            int64\n",
       "action_quotient                  float64\n",
       "sentiment_quotient               float64\n",
       "avg_flesch_reading_ease_class     object\n",
       "toxicity                         float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of unique ids of len(df) to anonymize authors\n",
    "ids = list(range(len(test)))\n",
    "random.shuffle(ids)\n",
    "\n",
    "# create dict to map authors to ids\n",
    "author_to_id = dict(zip(test['author'], ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_data = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(f'../data/samples/messages_sample_full.csv.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anonymize auhtor according to dict in both agg_data and data\n",
    "agg_data['author_id'] = agg_data['author'].map(author_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['author_id'] = data['author'].map(author_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>UID_key</th>\n",
       "      <th>author</th>\n",
       "      <th>fwd_message</th>\n",
       "      <th>fwd_author</th>\n",
       "      <th>date</th>\n",
       "      <th>group_or_channel</th>\n",
       "      <th>own_message</th>\n",
       "      <th>forwarded_message</th>\n",
       "      <th>group_name</th>\n",
       "      <th>fwd_message_string</th>\n",
       "      <th>final_message</th>\n",
       "      <th>final_message_string</th>\n",
       "      <th>author_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1930547378214939428038991</td>\n",
       "      <td>lord hol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>group</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Alles Ausser Mainstream Chat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Du wirst hier keine Meinungsfreiheit erfahren....</td>\n",
       "      <td>Du wirst hier keine Meinungsfreiheit erfahren....</td>\n",
       "      <td>6056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3440874592547339069585367</td>\n",
       "      <td>luis martinez</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>group</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1Research7Intelligence Room</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This character is presumed to be John Podesta,...</td>\n",
       "      <td>This character is presumed to be John Podesta,...</td>\n",
       "      <td>7132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3440904592547339069585367</td>\n",
       "      <td>andrew</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>group</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1Research7Intelligence Room</td>\n",
       "      <td>NaN</td>\n",
       "      <td>die feuerwehrfahrzeuge sehen aber nicht wie di...</td>\n",
       "      <td>die feuerwehrfahrzeuge sehen aber nicht wie di...</td>\n",
       "      <td>11501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3440914592547339069585367</td>\n",
       "      <td>d. zerone</td>\n",
       "      <td>Berlin-Reinickendorf 🧨 Neukölln 01.01.2021 4k ...</td>\n",
       "      <td>News ❤️</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>group</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1Research7Intelligence Room</td>\n",
       "      <td>Berlin-Reinickendorf  Neukölln 01.01.2021 4k _...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3440924592547339069585367</td>\n",
       "      <td>luis martinez</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>group</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1Research7Intelligence Room</td>\n",
       "      <td>NaN</td>\n",
       "      <td>weil ich für die Gruppe nicht beleidigen kann,...</td>\n",
       "      <td>weil ich für die Gruppe nicht beleidigen kann,...</td>\n",
       "      <td>7132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0                    UID_key         author  \\\n",
       "0             0           0  1930547378214939428038991       lord hol   \n",
       "1             1           1  3440874592547339069585367  luis martinez   \n",
       "2             2           2  3440904592547339069585367         andrew   \n",
       "3             3           3  3440914592547339069585367      d. zerone   \n",
       "4             4           4  3440924592547339069585367  luis martinez   \n",
       "\n",
       "                                         fwd_message fwd_author        date  \\\n",
       "0                                                NaN        NaN  2021-01-01   \n",
       "1                                                NaN        NaN  2021-01-01   \n",
       "2                                                NaN        NaN  2021-01-01   \n",
       "3  Berlin-Reinickendorf 🧨 Neukölln 01.01.2021 4k ...    News ❤️  2021-01-01   \n",
       "4                                                NaN        NaN  2021-01-01   \n",
       "\n",
       "  group_or_channel  own_message  forwarded_message  \\\n",
       "0            group            1                  0   \n",
       "1            group            1                  0   \n",
       "2            group            1                  0   \n",
       "3            group            0                  1   \n",
       "4            group            1                  0   \n",
       "\n",
       "                     group_name  \\\n",
       "0  Alles Ausser Mainstream Chat   \n",
       "1   1Research7Intelligence Room   \n",
       "2   1Research7Intelligence Room   \n",
       "3   1Research7Intelligence Room   \n",
       "4   1Research7Intelligence Room   \n",
       "\n",
       "                                  fwd_message_string  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  Berlin-Reinickendorf  Neukölln 01.01.2021 4k _...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                       final_message  \\\n",
       "0  Du wirst hier keine Meinungsfreiheit erfahren....   \n",
       "1  This character is presumed to be John Podesta,...   \n",
       "2  die feuerwehrfahrzeuge sehen aber nicht wie di...   \n",
       "3                                                NaN   \n",
       "4  weil ich für die Gruppe nicht beleidigen kann,...   \n",
       "\n",
       "                                final_message_string  author_id  \n",
       "0  Du wirst hier keine Meinungsfreiheit erfahren....       6056  \n",
       "1  This character is presumed to be John Podesta,...       7132  \n",
       "2  die feuerwehrfahrzeuge sehen aber nicht wie di...      11501  \n",
       "3                                                NaN       2266  \n",
       "4  weil ich für die Gruppe nicht beleidigen kann,...       7132  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename author_id to author and drop old auhtor column\n",
    "agg_data = agg_data.drop(columns='author', axis=1)\n",
    "agg_data = agg_data.rename(columns={'author_id': 'author'})\n",
    "\n",
    "data = data.drop(columns='author', axis=1)\n",
    "data = data.rename(columns={'author_id': 'author'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns='author', axis=1)\n",
    "data = data.rename(columns={'author_id': 'author'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>positive_sentiment</th>\n",
       "      <th>negative_sentiment</th>\n",
       "      <th>neutral_sentiment</th>\n",
       "      <th>channel_messages</th>\n",
       "      <th>group_messages</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_sent_length</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>exclamation_count</th>\n",
       "      <th>question_count</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>liwc_I</th>\n",
       "      <th>liwc_We</th>\n",
       "      <th>liwc_You</th>\n",
       "      <th>liwc_Other</th>\n",
       "      <th>liwc_Affect</th>\n",
       "      <th>final_message</th>\n",
       "      <th>final_message_string</th>\n",
       "      <th>own_message</th>\n",
       "      <th>forwarded_message</th>\n",
       "      <th>total_message_count</th>\n",
       "      <th>was_forwarded</th>\n",
       "      <th>own_message_count</th>\n",
       "      <th>forwarded_message_count</th>\n",
       "      <th>action_quotient</th>\n",
       "      <th>sentiment_quotient</th>\n",
       "      <th>avg_flesch_reading_ease_class</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>&lt;a href=\"https://youtu.be/i8SOiIPx-KQ\"&gt;https:/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unclassified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ANTIFA MOFU</td>\n",
       "      <td>ANTIFA MOFU</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>difficult</td>\n",
       "      <td>0.249241</td>\n",
       "      <td>9920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Wohin bist Du denn geflüchtet,  ist doch fast ...</td>\n",
       "      <td>Wohin bist Du denn geflüchtet, ist doch fast ü...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>easy</td>\n",
       "      <td>0.426917</td>\n",
       "      <td>8703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>76.500000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>6.595588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>47.725000</td>\n",
       "      <td>0.044972</td>\n",
       "      <td>0.007463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011194</td>\n",
       "      <td>0.056167</td>\n",
       "      <td>Ich wünsche gute \"Verdaulichkeit\". 😉 Blaues Ge...</td>\n",
       "      <td>Ich wünsche gute \"Verdaulichkeit\". Blaues Gemü...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>difficult</td>\n",
       "      <td>0.105663</td>\n",
       "      <td>13832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.027778</td>\n",
       "      <td>16.222222</td>\n",
       "      <td>4.616204</td>\n",
       "      <td>5.016480</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>82.231143</td>\n",
       "      <td>0.001736</td>\n",
       "      <td>0.004934</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.032971</td>\n",
       "      <td>0.072214</td>\n",
       "      <td>Na ja, andererseits sagte Trump und sein Gefol...</td>\n",
       "      <td>Na ja, andererseits sagte Trump und sein Gefol...</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>3.052632</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>easy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   noun_count  verb_count  adj_count  positive_sentiment  negative_sentiment  \\\n",
       "0         0.0         0.0        0.0            0.000000            0.000000   \n",
       "1         1.0         0.0        0.0            0.000000            1.000000   \n",
       "2         1.0         2.0        1.0            0.000000            1.000000   \n",
       "3        26.0        12.0       10.0            0.500000            0.000000   \n",
       "4        95.0        58.0       19.0            0.222222            0.361111   \n",
       "\n",
       "   neutral_sentiment  channel_messages  group_messages  sent_count  \\\n",
       "0           0.000000               0.0             1.0    0.000000   \n",
       "1           0.000000               0.0             1.0    1.000000   \n",
       "2           0.000000               0.0             1.0    2.000000   \n",
       "3           0.500000               0.0             1.0    7.500000   \n",
       "4           0.388889               0.0             1.0    3.027778   \n",
       "\n",
       "   word_count  avg_sent_length  avg_word_length  exclamation_count  \\\n",
       "0    0.000000         0.000000         0.000000           0.000000   \n",
       "1    2.000000         2.000000         5.000000           0.000000   \n",
       "2   14.000000         7.000000         4.857143           0.000000   \n",
       "3   76.500000         8.500000         6.595588           0.000000   \n",
       "4   16.222222         4.616204         5.016480           0.555556   \n",
       "\n",
       "   question_count  emoji_count  flesch_reading_ease    liwc_I   liwc_We  \\\n",
       "0        0.000000     0.000000                  NaN  0.000000  0.000000   \n",
       "1        0.000000     0.000000            31.750000  0.000000  0.000000   \n",
       "2        0.000000     0.000000            84.100000  0.000000  0.000000   \n",
       "3        0.500000     2.500000            47.725000  0.044972  0.007463   \n",
       "4        0.138889     0.055556            82.231143  0.001736  0.004934   \n",
       "\n",
       "   liwc_You  liwc_Other  liwc_Affect  \\\n",
       "0  0.000000    0.000000     0.000000   \n",
       "1  0.000000    0.000000     0.000000   \n",
       "2  0.071429    0.000000     0.071429   \n",
       "3  0.000000    0.011194     0.056167   \n",
       "4  0.003968    0.032971     0.072214   \n",
       "\n",
       "                                       final_message  \\\n",
       "0  <a href=\"https://youtu.be/i8SOiIPx-KQ\">https:/...   \n",
       "1                                 ANTIFA MOFU          \n",
       "2  Wohin bist Du denn geflüchtet,  ist doch fast ...   \n",
       "3  Ich wünsche gute \"Verdaulichkeit\". 😉 Blaues Ge...   \n",
       "4  Na ja, andererseits sagte Trump und sein Gefol...   \n",
       "\n",
       "                                final_message_string  own_message  \\\n",
       "0                                                NaN     0.571429   \n",
       "1                                       ANTIFA MOFU      1.000000   \n",
       "2  Wohin bist Du denn geflüchtet, ist doch fast ü...     1.000000   \n",
       "3  Ich wünsche gute \"Verdaulichkeit\". Blaues Gemü...     1.000000   \n",
       "4  Na ja, andererseits sagte Trump und sein Gefol...     0.947368   \n",
       "\n",
       "   forwarded_message  total_message_count  was_forwarded  own_message_count  \\\n",
       "0                0.0                    7              0                  4   \n",
       "1                0.0                    1              0                  1   \n",
       "2                0.0                    1              0                  1   \n",
       "3                0.0                    2              0                  2   \n",
       "4                0.0                   38              0                 36   \n",
       "\n",
       "   forwarded_message_count  action_quotient  sentiment_quotient  \\\n",
       "0                        0              NaN                 NaN   \n",
       "1                        0              NaN            0.000000   \n",
       "2                        0         2.000000            0.000000   \n",
       "3                        0         1.200000                 NaN   \n",
       "4                        0         3.052632            0.615385   \n",
       "\n",
       "  avg_flesch_reading_ease_class  toxicity  author  \n",
       "0                  unclassified       NaN    9885  \n",
       "1                     difficult  0.249241    9920  \n",
       "2                          easy  0.426917    8703  \n",
       "3                     difficult  0.105663   13832  \n",
       "4                          easy       NaN   14964  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_data.to_csv('../results/post-aggregation/author_full.csv.gzip', compression='gzip', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/samples/messages_sample_full.csv.gzip', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dict\n",
    "import json\n",
    "\n",
    "with open('../data/author_to_id.json', 'w') as f:\n",
    "    json.dump(author_to_id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/samples/messages_sample_full.csv.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>UID_key</th>\n",
       "      <th>fwd_message</th>\n",
       "      <th>fwd_author</th>\n",
       "      <th>date</th>\n",
       "      <th>group_or_channel</th>\n",
       "      <th>own_message</th>\n",
       "      <th>forwarded_message</th>\n",
       "      <th>group_name</th>\n",
       "      <th>fwd_message_string</th>\n",
       "      <th>final_message</th>\n",
       "      <th>final_message_string</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1930547378214939428038991</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>group</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Alles Ausser Mainstream Chat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Du wirst hier keine Meinungsfreiheit erfahren....</td>\n",
       "      <td>Du wirst hier keine Meinungsfreiheit erfahren....</td>\n",
       "      <td>6056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3440874592547339069585367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>group</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1Research7Intelligence Room</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This character is presumed to be John Podesta,...</td>\n",
       "      <td>This character is presumed to be John Podesta,...</td>\n",
       "      <td>7132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3440904592547339069585367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>group</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1Research7Intelligence Room</td>\n",
       "      <td>NaN</td>\n",
       "      <td>die feuerwehrfahrzeuge sehen aber nicht wie di...</td>\n",
       "      <td>die feuerwehrfahrzeuge sehen aber nicht wie di...</td>\n",
       "      <td>11501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3440914592547339069585367</td>\n",
       "      <td>Berlin-Reinickendorf 🧨 Neukölln 01.01.2021 4k ...</td>\n",
       "      <td>News ❤️</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>group</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1Research7Intelligence Room</td>\n",
       "      <td>Berlin-Reinickendorf  Neukölln 01.01.2021 4k _...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3440924592547339069585367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>group</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1Research7Intelligence Room</td>\n",
       "      <td>NaN</td>\n",
       "      <td>weil ich für die Gruppe nicht beleidigen kann,...</td>\n",
       "      <td>weil ich für die Gruppe nicht beleidigen kann,...</td>\n",
       "      <td>7132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0                    UID_key  \\\n",
       "0             0           0  1930547378214939428038991   \n",
       "1             1           1  3440874592547339069585367   \n",
       "2             2           2  3440904592547339069585367   \n",
       "3             3           3  3440914592547339069585367   \n",
       "4             4           4  3440924592547339069585367   \n",
       "\n",
       "                                         fwd_message fwd_author        date  \\\n",
       "0                                                NaN        NaN  2021-01-01   \n",
       "1                                                NaN        NaN  2021-01-01   \n",
       "2                                                NaN        NaN  2021-01-01   \n",
       "3  Berlin-Reinickendorf 🧨 Neukölln 01.01.2021 4k ...    News ❤️  2021-01-01   \n",
       "4                                                NaN        NaN  2021-01-01   \n",
       "\n",
       "  group_or_channel  own_message  forwarded_message  \\\n",
       "0            group            1                  0   \n",
       "1            group            1                  0   \n",
       "2            group            1                  0   \n",
       "3            group            0                  1   \n",
       "4            group            1                  0   \n",
       "\n",
       "                     group_name  \\\n",
       "0  Alles Ausser Mainstream Chat   \n",
       "1   1Research7Intelligence Room   \n",
       "2   1Research7Intelligence Room   \n",
       "3   1Research7Intelligence Room   \n",
       "4   1Research7Intelligence Room   \n",
       "\n",
       "                                  fwd_message_string  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  Berlin-Reinickendorf  Neukölln 01.01.2021 4k _...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                       final_message  \\\n",
       "0  Du wirst hier keine Meinungsfreiheit erfahren....   \n",
       "1  This character is presumed to be John Podesta,...   \n",
       "2  die feuerwehrfahrzeuge sehen aber nicht wie di...   \n",
       "3                                                NaN   \n",
       "4  weil ich für die Gruppe nicht beleidigen kann,...   \n",
       "\n",
       "                                final_message_string  author  \n",
       "0  Du wirst hier keine Meinungsfreiheit erfahren....    6056  \n",
       "1  This character is presumed to be John Podesta,...    7132  \n",
       "2  die feuerwehrfahrzeuge sehen aber nicht wie di...   11501  \n",
       "3                                                NaN    2266  \n",
       "4  weil ich für die Gruppe nicht beleidigen kann,...    7132  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fd/qf26p3js3x32_9_qwsb841y80000gn/T/ipykernel_22926/1351491980.py:1: DtypeWarning: Columns (21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_agg_data = pd.read_csv('../results/post-aggregation/author_full.csv.gzip', compression='gzip')\n"
     ]
    }
   ],
   "source": [
    "test_agg_data = pd.read_csv('../results/post-aggregation/author_full.csv.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>positive_sentiment</th>\n",
       "      <th>negative_sentiment</th>\n",
       "      <th>neutral_sentiment</th>\n",
       "      <th>channel_messages</th>\n",
       "      <th>group_messages</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_sent_length</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>exclamation_count</th>\n",
       "      <th>question_count</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>liwc_I</th>\n",
       "      <th>liwc_We</th>\n",
       "      <th>liwc_You</th>\n",
       "      <th>liwc_Other</th>\n",
       "      <th>liwc_Affect</th>\n",
       "      <th>final_message</th>\n",
       "      <th>final_message_string</th>\n",
       "      <th>own_message</th>\n",
       "      <th>forwarded_message</th>\n",
       "      <th>total_message_count</th>\n",
       "      <th>was_forwarded</th>\n",
       "      <th>own_message_count</th>\n",
       "      <th>forwarded_message_count</th>\n",
       "      <th>action_quotient</th>\n",
       "      <th>sentiment_quotient</th>\n",
       "      <th>avg_flesch_reading_ease_class</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>&lt;a href=\"https://youtu.be/i8SOiIPx-KQ\"&gt;https:/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unclassified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ANTIFA MOFU</td>\n",
       "      <td>ANTIFA MOFU</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>difficult</td>\n",
       "      <td>0.249241</td>\n",
       "      <td>9920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Wohin bist Du denn geflüchtet,  ist doch fast ...</td>\n",
       "      <td>Wohin bist Du denn geflüchtet, ist doch fast ü...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>easy</td>\n",
       "      <td>0.426917</td>\n",
       "      <td>8703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>76.500000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>6.595588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>47.725000</td>\n",
       "      <td>0.044972</td>\n",
       "      <td>0.007463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011194</td>\n",
       "      <td>0.056167</td>\n",
       "      <td>Ich wünsche gute \"Verdaulichkeit\". 😉 Blaues Ge...</td>\n",
       "      <td>Ich wünsche gute \"Verdaulichkeit\". Blaues Gemü...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>difficult</td>\n",
       "      <td>0.105663</td>\n",
       "      <td>13832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.027778</td>\n",
       "      <td>16.222222</td>\n",
       "      <td>4.616204</td>\n",
       "      <td>5.016480</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>82.231143</td>\n",
       "      <td>0.001736</td>\n",
       "      <td>0.004934</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.032971</td>\n",
       "      <td>0.072214</td>\n",
       "      <td>Na ja, andererseits sagte Trump und sein Gefol...</td>\n",
       "      <td>Na ja, andererseits sagte Trump und sein Gefol...</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>3.052632</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>easy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   noun_count  verb_count  adj_count  positive_sentiment  negative_sentiment  \\\n",
       "0         0.0         0.0        0.0            0.000000            0.000000   \n",
       "1         1.0         0.0        0.0            0.000000            1.000000   \n",
       "2         1.0         2.0        1.0            0.000000            1.000000   \n",
       "3        26.0        12.0       10.0            0.500000            0.000000   \n",
       "4        95.0        58.0       19.0            0.222222            0.361111   \n",
       "\n",
       "   neutral_sentiment  channel_messages  group_messages  sent_count  \\\n",
       "0           0.000000               0.0             1.0    0.000000   \n",
       "1           0.000000               0.0             1.0    1.000000   \n",
       "2           0.000000               0.0             1.0    2.000000   \n",
       "3           0.500000               0.0             1.0    7.500000   \n",
       "4           0.388889               0.0             1.0    3.027778   \n",
       "\n",
       "   word_count  avg_sent_length  avg_word_length  exclamation_count  \\\n",
       "0    0.000000         0.000000         0.000000           0.000000   \n",
       "1    2.000000         2.000000         5.000000           0.000000   \n",
       "2   14.000000         7.000000         4.857143           0.000000   \n",
       "3   76.500000         8.500000         6.595588           0.000000   \n",
       "4   16.222222         4.616204         5.016480           0.555556   \n",
       "\n",
       "   question_count  emoji_count  flesch_reading_ease    liwc_I   liwc_We  \\\n",
       "0        0.000000     0.000000                  NaN  0.000000  0.000000   \n",
       "1        0.000000     0.000000            31.750000  0.000000  0.000000   \n",
       "2        0.000000     0.000000            84.100000  0.000000  0.000000   \n",
       "3        0.500000     2.500000            47.725000  0.044972  0.007463   \n",
       "4        0.138889     0.055556            82.231143  0.001736  0.004934   \n",
       "\n",
       "   liwc_You  liwc_Other  liwc_Affect  \\\n",
       "0  0.000000    0.000000     0.000000   \n",
       "1  0.000000    0.000000     0.000000   \n",
       "2  0.071429    0.000000     0.071429   \n",
       "3  0.000000    0.011194     0.056167   \n",
       "4  0.003968    0.032971     0.072214   \n",
       "\n",
       "                                       final_message  \\\n",
       "0  <a href=\"https://youtu.be/i8SOiIPx-KQ\">https:/...   \n",
       "1                                 ANTIFA MOFU          \n",
       "2  Wohin bist Du denn geflüchtet,  ist doch fast ...   \n",
       "3  Ich wünsche gute \"Verdaulichkeit\". 😉 Blaues Ge...   \n",
       "4  Na ja, andererseits sagte Trump und sein Gefol...   \n",
       "\n",
       "                                final_message_string  own_message  \\\n",
       "0                                                NaN     0.571429   \n",
       "1                                       ANTIFA MOFU      1.000000   \n",
       "2  Wohin bist Du denn geflüchtet, ist doch fast ü...     1.000000   \n",
       "3  Ich wünsche gute \"Verdaulichkeit\". Blaues Gemü...     1.000000   \n",
       "4  Na ja, andererseits sagte Trump und sein Gefol...     0.947368   \n",
       "\n",
       "   forwarded_message  total_message_count  was_forwarded  own_message_count  \\\n",
       "0                0.0                    7              0                  4   \n",
       "1                0.0                    1              0                  1   \n",
       "2                0.0                    1              0                  1   \n",
       "3                0.0                    2              0                  2   \n",
       "4                0.0                   38              0                 36   \n",
       "\n",
       "   forwarded_message_count  action_quotient  sentiment_quotient  \\\n",
       "0                        0              NaN                 NaN   \n",
       "1                        0              NaN            0.000000   \n",
       "2                        0         2.000000            0.000000   \n",
       "3                        0         1.200000                 NaN   \n",
       "4                        0         3.052632            0.615385   \n",
       "\n",
       "  avg_flesch_reading_ease_class  toxicity  author  \n",
       "0                  unclassified       NaN    9885  \n",
       "1                     difficult  0.249241    9920  \n",
       "2                          easy  0.426917    8703  \n",
       "3                     difficult  0.105663   13832  \n",
       "4                          easy       NaN   14964  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_agg_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get author ids that are in agg_data but not in data\n",
    "missing_authors = set(test_agg_data['author']) - set(test_data['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_authors = test_data.groupby('group_name')['author'].apply(set)\n",
    "\n",
    "# get unique authors and map them to indices\n",
    "authors = sorted(set(str(author) for author in test_data['author']))\n",
    "author_idx_map = {author: idx for idx, author in enumerate(authors)}\n",
    "\n",
    "# Check for missing authors\n",
    "missing_authors = [author for author in authors if author not in author_idx_map]\n",
    "if missing_authors:\n",
    "    print(f\"Missing authors in author_idx_map: {missing_authors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get combinations of two authors in each group\n",
    "edges = []\n",
    "for authors_in_group in grouped_authors:\n",
    "    if len(authors_in_group) > 1:\n",
    "        edges += combinations(authors_in_group, 2)\n",
    "\n",
    "# count occurrences of each combination to determine edge weight\n",
    "edge_weights = Counter(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists for COO sparse matrix format (row, col, data)\n",
    "row_indices = []\n",
    "col_indices = []\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "16384",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (author_1, author_2), weight \u001b[38;5;129;01min\u001b[39;00m edge_weights\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 2\u001b[0m     idx_1 \u001b[38;5;241m=\u001b[39m \u001b[43mauthor_idx_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mauthor_1\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      3\u001b[0m     idx_2 \u001b[38;5;241m=\u001b[39m author_idx_map[author_2]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Add both directions since the matrix is symmetric\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 16384"
     ]
    }
   ],
   "source": [
    "for (author_1, author_2), weight in edge_weights.items():\n",
    "    idx_1 = author_idx_map[author_1]\n",
    "    idx_2 = author_idx_map[author_2]\n",
    "\n",
    "    # Add both directions since the matrix is symmetric\n",
    "    row_indices.append(idx_1)\n",
    "    col_indices.append(idx_2)\n",
    "    data.append(weight)\n",
    "    \n",
    "    row_indices.append(idx_2)\n",
    "    col_indices.append(idx_1)\n",
    "    data.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_test = author_idx_map['16384']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix created.\n",
      "Feature tensor shape: torch.Size([16885, 29])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2492],\n",
       "        [2.0000, 1.0000, 0.0000,  ..., 2.0000, 0.0000, 0.4269],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_feature_matrix(agg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = test_agg_data.fillna(0)\n",
    "# Create empty lists for COO sparse matrix format (row, col, data)\n",
    "row_indices = []\n",
    "col_indices = []\n",
    "data = []\n",
    "feature_columns = dataset.columns\n",
    "feature_columns = [feat for feat in feature_columns if (feat != 'final_message_string') & (feat != 'final_message') & (feat != 'author') & (feat != 'avg_flesch_reading_ease_class')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
