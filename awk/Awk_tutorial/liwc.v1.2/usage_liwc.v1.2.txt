#Text has to be in the format of one piece (e.g. one tweet) per line, make sure that there are no new line characters in a line before you export for example with a gsub (to check that run "wc -l TEXT" on the file and see if the line count matches the number of rows in R or the number of JSON objects)

#Tokenize
awk -vORS="\t" -F" " '{gsub(/^"|"$|""|\.|\,|\!|\?|\:|\;|‘|"|'\''|&amp|“|”|-|➡️|→|↓|http[a-zA-Z0-9\:/=?.-_+&]+|#[a-zA-Z0-9]+|@[a-zA-Z0-9:_'\''’]+|…|\\n|\\r|\(|\)|NA|\*|\<|\>|„|\/|\\|>>|^^/,"");gsub(/\s{2,}/," ");split($0,m); for(i in m) print tolower($i);printf "\n"}' TEXT | sed s'/\t$//' > TEXT_TOKENIZED

#Run on tokenized text
#Excluded words have to be comma-seperated like -vexclude="einkaufen,ah,fahrkarte", omit the "*" for wildcard words
awk -vexclude="" -f liwc_multi.v1.2.awk liwc_german_long_all TEXT_TOKENIZED > OUTFILE

#Run on tokenized text and show matches (to sanity check and compute word clouds)
#Excluded words have to be comma-seperated like -vexclude="einkaufen,ah,fahrkarte", omit the "*" for wildcard words
awk -vexclude="" -f liwc_multi_showmatches.v1.2.awk liwc_german_long_all TEXT_TOKENIZED > OUTFILE_MATCHES

#You can exchange the dictionary for the english version, everything works the same way, both lexica are the 2015 version