# tutorialspoint
- long tutorial: https://www.tutorialspoint.com/awk/awk_basic_syntax.htm
- many one liners explained, good for the basics: https://catonmat.net/awk-one-liners-explained-part-one

#tokenizing
./tokenize.sh text.tmp

#or run the tokenizer directly
awk -F"\t" '{print $1,$2}' text.tmp | awk -vORS="\t" -F" " '{gsub(/^"|"$|""|\.|\,|\!|\?|\:|\;|‘|"|'\''|&amp|“|”|-|➡️|→|↓|http[a-zA-Z0-9\:/=?.-_+&]+|#[a-zA-Z0-9]+|@[a-zA-Z0-9:_'\''’]+|…|\\n|\\r|\(|\)|NA|\*|\<|\>|„|\/|\\|>>|^^/,"");gsub(/\s{2,}/," ");split($0,m); for(i in m) print tolower($i);printf "\n"}' - | sed s'/\t$//' > tokenized_text.tmp

#match only one category and count matches
awk -f dictanalysis.v1.5_easier.awk anger_terms tokenized_text.tmp

#match all categories in lexicon and count matches
#use sed to remove the last unnecessary delimiter at the end of each line
awk -f liwc_multi.v1.1.awk dic_german_excluded tokenized_text.tmp | sed 's/\t$//'

#match all categories in lexicon and show matches and their frequency
awk -f liwc_multi_showmatches.v1.awk dic_german_excluded tokenized_text.tmp 


additional useful tools:

https://stedolan.github.io/jq/

https://www.gnu.org/software/parallel/

#parallel processing with xargs
ls -1 tokenized_text.tmp* | xargs -I{} -n 1 -P 4 sh -c "awk -f liwc_multi.v1.1.awk dic_german_excluded {} | sed "s/\t$//" > {}_out"


#stemming, not needed for liwc style analysis
Download stemmer here:
https://tartarus.org/martin/PorterStemmer/awk.txt

Run stemmer:
awk -F"\t" -f awk.txt tokenized_text.tmp > stemmed_tokenized_text.tmp

Length of tweets
awk '{print length}' tweet_text > tweet_length & 
